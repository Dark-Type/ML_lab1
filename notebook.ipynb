{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T08:40:18.552382Z",
     "start_time": "2025-03-10T08:40:06.772547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install catboost\n",
    "!pip install optuna"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in ./.venv/lib/python3.9/site-packages (1.2.7)\r\n",
      "Requirement already satisfied: graphviz in ./.venv/lib/python3.9/site-packages (from catboost) (0.20.3)\r\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (from catboost) (3.9.4)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in ./.venv/lib/python3.9/site-packages (from catboost) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=0.24 in ./.venv/lib/python3.9/site-packages (from catboost) (2.2.3)\r\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from catboost) (1.13.1)\r\n",
      "Requirement already satisfied: plotly in ./.venv/lib/python3.9/site-packages (from catboost) (6.0.0)\r\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.9/site-packages (from catboost) (1.17.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas>=0.24->catboost) (2025.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->catboost) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib->catboost) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->catboost) (4.56.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->catboost) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->catboost) (24.2)\r\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib->catboost) (11.1.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->catboost) (3.2.1)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->catboost) (6.5.2)\r\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./.venv/lib/python3.9/site-packages (from plotly->catboost) (1.29.1)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.21.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting optuna\r\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/28/09/c4d329f7969443cdd4d482048ca406b6f61cda3c8e99ace71feaec7c8734/optuna-4.2.1-py3-none-any.whl.metadata\r\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\r\n",
      "Collecting alembic>=1.5.0 (from optuna)\r\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/99/f7/d398fae160568472ddce0b3fde9c4581afc593019a6adc91006a66406991/alembic-1.15.1-py3-none-any.whl.metadata\r\n",
      "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting colorlog (from optuna)\r\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/e3/51/9b208e85196941db2f0654ad0357ca6388ab3ed67efdbfc799f35d1f83aa/colorlog-6.9.0-py3-none-any.whl.metadata\r\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from optuna) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from optuna) (24.2)\r\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\r\n",
      "  Obtaining dependency information for sqlalchemy>=1.4.2 from https://files.pythonhosted.org/packages/f5/76/297c532ea77c90e858a2967ba8ed62e8d9c503edc968b0c875361631cf1f/SQLAlchemy-2.0.38-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading SQLAlchemy-2.0.38-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.6 kB)\r\n",
      "Collecting tqdm (from optuna)\r\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\r\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.7/57.7 kB\u001B[0m \u001B[31m411.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: PyYAML in ./.venv/lib/python3.9/site-packages (from optuna) (6.0.2)\r\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\r\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/cd/83/de0a49e7de540513f53ab5d2e105321dedeb08a8f5850f0208decf4390ec/Mako-1.3.9-py3-none-any.whl.metadata\r\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12 in ./.venv/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./.venv/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\r\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m383.6/383.6 kB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m231.8/231.8 kB\u001B[0m \u001B[31m390.4 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading SQLAlchemy-2.0.38-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m0m\r\n",
      "\u001B[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\r\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading Mako-1.3.9-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tqdm, sqlalchemy, Mako, colorlog, alembic, optuna\r\n",
      "Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1 sqlalchemy-2.0.38 tqdm-4.67.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T08:41:53.655828Z",
     "start_time": "2025-03-10T08:41:46.920965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install xgboost\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ],
   "id": "14b889d84a930772",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.9/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib/python3.9/site-packages (from seaborn) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.9/site-packages (from seaborn) (2.2.3)\r\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.venv/lib/python3.9/site-packages (from seaborn) (3.9.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\r\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.5.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2025.1)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.21.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (1.6.1)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: xgboost in ./.venv/lib/python3.9/site-packages (2.1.4)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from xgboost) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from xgboost) (1.13.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T14:11:53.394661Z",
     "start_time": "2025-03-09T14:11:53.320975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = pd.read_csv('/Users/nikitaskazutin/PyCharmMiscProject/titanic_spaceship_dataset/train.csv')\n",
    "test_data = pd.read_csv('/Users/nikitaskazutin/PyCharmMiscProject/titanic_spaceship_dataset/test.csv')"
   ],
   "id": "2096b1bb6184ad6d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T14:12:52.505921Z",
     "start_time": "2025-03-09T14:12:52.480729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_spaceship_data(train_data, test_data, val_size=0.2, random_state=42):\n",
    "\n",
    "    print(\"Loading and preparing Spaceship Titanic data...\")\n",
    "\n",
    "\n",
    "    print(f\"Training data: {train_data.shape[0]} rows, {train_data.shape[1]} columns\")\n",
    "    print(f\"Test data: {test_data.shape[0]} rows, {test_data.shape[1]} columns\")\n",
    "\n",
    "    train_ids = train_data['PassengerId'].copy() if 'PassengerId' in train_data.columns else None\n",
    "    test_ids = test_data['PassengerId'].copy() if 'PassengerId' in test_data.columns else None\n",
    "\n",
    "    if 'Transported' in train_data.columns:\n",
    "        y = train_data['Transported'].copy()\n",
    "        if y.dtype == bool:\n",
    "            y = y.astype(int)\n",
    "            print(\"Converted boolean target to integer (0/1)\")\n",
    "        X = train_data.drop('Transported', axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"No 'Transported' column found in training data\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=val_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Split into {X_train.shape[0]} training samples and {X_val.shape[0]} validation samples\")\n",
    "\n",
    "    dfs = [\n",
    "        (X_train, 'train'),\n",
    "        (X_val, 'val'),\n",
    "        (test_data, 'test')\n",
    "    ]\n",
    "\n",
    "    combined = pd.concat([df.assign(source=source) for df, source in dfs], axis=0)\n",
    "\n",
    "\n",
    "    if 'PassengerId' in combined.columns:\n",
    "        combined['PassengerGroup'] = combined['PassengerId'].str.split('_').str[0].astype(int)\n",
    "        combined['PassengerNumber'] = combined['PassengerId'].str.split('_').str[1].astype(int)\n",
    "\n",
    "\n",
    "        group_sizes = combined['PassengerGroup'].value_counts()\n",
    "        combined['GroupSize'] = combined['PassengerGroup'].map(group_sizes)\n",
    "        combined['TravelingAlone'] = (combined['GroupSize'] == 1).astype(int)\n",
    "\n",
    "    if 'Cabin' in combined.columns:\n",
    "        combined['CabinDeck'] = 'Unknown'\n",
    "        combined['CabinNum'] = np.nan\n",
    "        combined['CabinSide'] = 'Unknown'\n",
    "        combined['HasCabin'] = combined['Cabin'].notna().astype(int)\n",
    "\n",
    "\n",
    "        cabin_mask = combined['Cabin'].notna()\n",
    "        if cabin_mask.any():\n",
    "            cabin_parts = combined.loc[cabin_mask, 'Cabin'].str.split('/', expand=True)\n",
    "            if cabin_parts.shape[1] >= 3:\n",
    "                combined.loc[cabin_mask, 'CabinDeck'] = cabin_parts[0]\n",
    "                combined.loc[cabin_mask, 'CabinNum'] = pd.to_numeric(cabin_parts[1], errors='coerce')\n",
    "                combined.loc[cabin_mask, 'CabinSide'] = cabin_parts[2]\n",
    "\n",
    "    if 'Name' in combined.columns:\n",
    "        combined['LastName'] = combined['Name'].str.split(' ').str[0]\n",
    "        combined['FirstName'] = combined['Name'].str.split(' ').str[1:].str.join(' ')\n",
    "\n",
    "        family_sizes = combined['LastName'].value_counts()\n",
    "        combined['FamilySize'] = combined['LastName'].map(family_sizes)\n",
    "        combined['HasFamily'] = (combined['FamilySize'] > 1).astype(int)\n",
    "\n",
    "    num_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    for col in num_cols:\n",
    "        if col in combined.columns:\n",
    "            group_medians = combined.groupby('PassengerGroup')[col].transform('median')\n",
    "            combined[col] = combined[col].fillna(group_medians)\n",
    "\n",
    "            if 'HomePlanet' in combined.columns:\n",
    "                planet_medians = combined.groupby('HomePlanet')[col].transform('median')\n",
    "                combined[col] = combined[col].fillna(planet_medians)\n",
    "\n",
    "            combined[col] = combined[col].fillna(combined[col].median())\n",
    "\n",
    "    spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    spending_cols = [col for col in spending_cols if col in combined.columns]\n",
    "    if spending_cols:\n",
    "        combined['TotalSpend'] = combined[spending_cols].sum(axis=1)\n",
    "        combined['HasSpent'] = (combined['TotalSpend'] > 0).astype(int)\n",
    "\n",
    "        combined['TotalSpendLog'] = np.log1p(combined['TotalSpend'])\n",
    "        for col in spending_cols:\n",
    "            combined[f'{col}Log'] = np.log1p(combined[col])\n",
    "\n",
    "        for col in spending_cols:\n",
    "            combined[f'{col}Ratio'] = 0\n",
    "            spend_mask = combined['TotalSpend'] > 0\n",
    "            if spend_mask.any():\n",
    "                combined.loc[spend_mask, f'{col}Ratio'] = combined.loc[spend_mask, col] / combined.loc[spend_mask, 'TotalSpend']\n",
    "\n",
    "        for col in spending_cols:\n",
    "            combined[f'Used{col}'] = (combined[col] > 0).astype(int)\n",
    "\n",
    "    cat_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "    cat_cols = [col for col in cat_cols if col in combined.columns]\n",
    "\n",
    "    for col in cat_cols:\n",
    "        for group in combined[combined[col].isna()]['PassengerGroup'].unique():\n",
    "            group_mode = combined[(combined['PassengerGroup'] == group) & combined[col].notna()][col].mode()\n",
    "            if not group_mode.empty:\n",
    "                combined.loc[(combined['PassengerGroup'] == group) & combined[col].isna(), col] = group_mode.iloc[0]\n",
    "\n",
    "        combined[col] = combined[col].fillna(combined[col].mode().iloc[0])\n",
    "\n",
    "    if 'Age' in combined.columns:\n",
    "        combined['AgeGroup'] = pd.cut(\n",
    "            combined['Age'],\n",
    "            bins=[0, 12, 18, 25, 40, 60, 1000],\n",
    "            labels=['Child', 'Teen', 'YoungAdult', 'Adult', 'MiddleAge', 'Senior']\n",
    "        )\n",
    "\n",
    "    if 'CryoSleep' in combined.columns and 'TotalSpend' in combined.columns:\n",
    "        combined['ActiveSpender'] = ((combined['CryoSleep'] == False) & (combined['TotalSpend'] > 0)).astype(int)\n",
    "        combined['CryoSleepSpender'] = ((combined['CryoSleep'] == True) & (combined['TotalSpend'] > 0)).astype(int)\n",
    "\n",
    "    if 'HomePlanet' in combined.columns and 'Destination' in combined.columns:\n",
    "        combined['Route'] = combined['HomePlanet'].astype(str) + \"_to_\" + combined['Destination'].astype(str)\n",
    "\n",
    "    cols_to_drop = ['Name', 'Cabin', 'PassengerId']\n",
    "    cols_to_drop = [col for col in cols_to_drop if col in combined.columns]\n",
    "    combined = combined.drop(cols_to_drop, axis=1)\n",
    "    cat_cols_to_encode = [\n",
    "        col for col in combined.columns\n",
    "        if col != 'source' and (\n",
    "            combined[col].dtype == 'object' or\n",
    "            isinstance(combined[col].dtype, pd.CategoricalDtype)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    combined = pd.get_dummies(combined, columns=cat_cols_to_encode, drop_first=True)\n",
    "\n",
    "    print(f\"After preprocessing: {combined.shape[1]-1} features created\")\n",
    "\n",
    "    X_train_proc = combined[combined['source'] == 'train'].drop('source', axis=1)\n",
    "    X_val_proc = combined[combined['source'] == 'val'].drop('source', axis=1)\n",
    "    X_test_proc = combined[combined['source'] == 'test'].drop('source', axis=1)\n",
    "\n",
    "    all_cols = sorted(list(set(X_train_proc.columns) |\n",
    "                          set(X_val_proc.columns) |\n",
    "                          set(X_test_proc.columns)))\n",
    "\n",
    "    for col in all_cols:\n",
    "        if col not in X_train_proc.columns:\n",
    "            X_train_proc[col] = 0\n",
    "        if col not in X_val_proc.columns:\n",
    "            X_val_proc[col] = 0\n",
    "        if col not in X_test_proc.columns:\n",
    "            X_test_proc[col] = 0\n",
    "\n",
    "    for name, df in [('X_train_proc', X_train_proc), ('X_val_proc', X_val_proc), ('X_test_proc', X_test_proc)]:\n",
    "        nan_cols = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "        if nan_cols:\n",
    "            print(f\"Found NaN values in {name} for columns: {nan_cols}\")\n",
    "            print(\"Filling remaining NaNs with appropriate values...\")\n",
    "\n",
    "            for col in nan_cols:\n",
    "                if np.issubdtype(df[col].dtype, np.number):\n",
    "                    fill_value = df[col].median()\n",
    "                    if pd.isna(fill_value):\n",
    "                        fill_value = 0\n",
    "                    df[col] = df[col].fillna(fill_value)\n",
    "                    print(f\"  - {col}: filled with median ({fill_value})\")\n",
    "                else:\n",
    "                    fill_value = df[col].mode().iloc[0] if not df[col].mode().empty else \"Unknown\"\n",
    "                    df[col] = df[col].fillna(fill_value)\n",
    "                    print(f\"  - {col}: filled with mode ({fill_value})\")\n",
    "\n",
    "    if 'Age' in combined.columns:\n",
    "        combined['Age_squared'] = combined['Age'] ** 2\n",
    "\n",
    "    if 'Age' in combined.columns and 'CryoSleep' in combined.columns:\n",
    "        combined['Age_CryoSleep'] = combined['Age'] * combined['CryoSleep'].astype(int)\n",
    "\n",
    "    spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    spending_cols = [col for col in spending_cols if col in combined.columns]\n",
    "\n",
    "    if spending_cols and 'GroupSize' in combined.columns:\n",
    "        combined['SpendPerPerson'] = combined['TotalSpend'] / combined['GroupSize']\n",
    "\n",
    "        combined['SpendingDiversity'] = (combined[spending_cols] > 0).sum(axis=1)\n",
    "\n",
    "        combined['SpendingPercentile'] = pd.qcut(combined['TotalSpend'].rank(method='first'),\n",
    "                                                q=10, labels=False, duplicates='drop')\n",
    "\n",
    "    planet_map = {'Earth': 0, 'Europa': 1, 'Mars': 2}\n",
    "    dest_map = {'TRAPPIST-1e': 0, 'PSO J318.5-22': 1, '55 Cancri e': 2}\n",
    "\n",
    "    if 'HomePlanet' in combined.columns and 'Destination' in combined.columns:\n",
    "        combined['HomePlanetNum'] = combined['HomePlanet'].map(planet_map)\n",
    "        combined['DestinationNum'] = combined['Destination'].map(dest_map)\n",
    "\n",
    "        combined['HomePlanetNum'] = combined['HomePlanetNum'].fillna(-1)\n",
    "        combined['DestinationNum'] = combined['DestinationNum'].fillna(-1)\n",
    "\n",
    "        combined['TravelDistance'] = abs(combined['DestinationNum'] - combined['HomePlanetNum'])\n",
    "\n",
    "    if 'VIP' in combined.columns and 'TotalSpend' in combined.columns:\n",
    "        combined['VIP_Spending'] = combined['VIP'].astype(int) * combined['TotalSpend']\n",
    "    all_clean = all([\n",
    "        not X_train_proc.isna().any().any(),\n",
    "        not X_val_proc.isna().any().any(),\n",
    "        not X_test_proc.isna().any().any()\n",
    "    ])\n",
    "\n",
    "    if all_clean:\n",
    "        print(\"All datasets are now free of NaN values!\")\n",
    "    else:\n",
    "        print(\"WARNING: There are still NaN values remaining. Check your data processing.\")\n",
    "\n",
    "    X_train_proc = X_train_proc[sorted(X_train_proc.columns)]\n",
    "    X_val_proc = X_val_proc[sorted(X_val_proc.columns)]\n",
    "    X_test_proc = X_test_proc[sorted(X_test_proc.columns)]\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(X_train_proc)\n",
    "\n",
    "\n",
    "    X_train_proc = pd.DataFrame(\n",
    "        scaler.transform(X_train_proc),\n",
    "        columns=X_train_proc.columns,\n",
    "        index=X_train_proc.index\n",
    "    )\n",
    "\n",
    "    X_val_proc = pd.DataFrame(\n",
    "        scaler.transform(X_val_proc),\n",
    "        columns=X_val_proc.columns,\n",
    "        index=X_val_proc.index\n",
    "    )\n",
    "\n",
    "    X_test_proc = pd.DataFrame(\n",
    "        scaler.transform(X_test_proc),\n",
    "        columns=X_test_proc.columns,\n",
    "        index=X_test_proc.index\n",
    "    )\n",
    "\n",
    "    print(\"Data scaling completed with StandardScaler\")\n",
    "    print(f\"Processed training features: {X_train_proc.shape}\")\n",
    "    print(f\"Processed validation features: {X_val_proc.shape}\")\n",
    "    print(f\"Processed test features: {X_test_proc.shape}\")\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train_proc,\n",
    "        'X_val': X_val_proc,\n",
    "        'X_test': X_test_proc,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'train_ids': train_ids,\n",
    "        'test_ids': test_ids\n",
    "    }\n"
   ],
   "id": "2e43cc56c344a98d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T05:29:37.317389Z",
     "start_time": "2025-03-10T05:29:37.301829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_base_models(data_dict, use_stratified_cv=True, cv_folds=5):\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "    X_train = data_dict['X_train']\n",
    "    X_val = data_dict['X_val']\n",
    "    y_train = data_dict['y_train']\n",
    "    y_val = data_dict['y_val']\n",
    "\n",
    "    print(\"Training individual base models...\")\n",
    "\n",
    "    models = {\n",
    "        'logistic': LogisticRegression(\n",
    "            C=0.1,\n",
    "            max_iter=1000,\n",
    "            solver='liblinear',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'random_forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "        'xgboost': xgb.XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42),\n",
    "        'catboost': cb.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, random_state=42, verbose=0)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if use_stratified_cv:\n",
    "        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        print(f\"Using {cv_folds}-fold StratifiedKFold cross-validation\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        accuracy = accuracy_score(y_val, val_pred)\n",
    "        roc_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "        if use_stratified_cv:\n",
    "            cv_probs = cross_val_predict(\n",
    "                model, X_train, y_train,\n",
    "                cv=skf,\n",
    "                method='predict_proba'\n",
    "            )[:, 1]\n",
    "\n",
    "            cv_auc = roc_auc_score(y_train, cv_probs)\n",
    "\n",
    "            overfit_gap = roc_auc - cv_auc\n",
    "\n",
    "            print(f\"CV ROC AUC: {cv_auc:.4f}\")\n",
    "            print(f\"Validation ROC AUC: {roc_auc:.4f}\")\n",
    "            print(f\"Potential overfitting: {overfit_gap:.4f}\")\n",
    "\n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'val_accuracy': accuracy,\n",
    "                'val_auc': roc_auc,\n",
    "                'cv_auc': cv_auc,\n",
    "                'overfit_gap': overfit_gap,\n",
    "                'val_predictions': val_pred,\n",
    "                'val_probabilities': val_prob\n",
    "            }\n",
    "        else:\n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'val_accuracy': accuracy,\n",
    "                'val_auc': roc_auc,\n",
    "                'val_predictions': val_pred,\n",
    "                'val_probabilities': val_prob\n",
    "            }\n",
    "\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation ROC AUC: {roc_auc:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, val_pred))\n",
    "\n",
    "    comparison_columns = ['Model', 'Accuracy', 'ROC AUC']\n",
    "    if use_stratified_cv:\n",
    "        comparison_columns.extend(['CV AUC', 'Overfit Gap'])\n",
    "\n",
    "    comparison_data = {'Model': list(results.keys())}\n",
    "    comparison_data['Accuracy'] = [results[m]['val_accuracy'] for m in results]\n",
    "    comparison_data['ROC AUC'] = [results[m]['val_auc'] for m in results]\n",
    "\n",
    "    if use_stratified_cv:\n",
    "        comparison_data['CV AUC'] = [results[m]['cv_auc'] for m in results]\n",
    "        comparison_data['Overfit Gap'] = [results[m]['overfit_gap'] for m in results]\n",
    "\n",
    "    comparison = pd.DataFrame(comparison_data).sort_values('ROC AUC', ascending=False)\n",
    "\n",
    "    print(\"\\n=== Model Comparison ===\")\n",
    "    print(comparison)\n",
    "\n",
    "    return results"
   ],
   "id": "3b3ae33130dc18ff",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T14:13:19.418383Z",
     "start_time": "2025-03-09T14:13:19.402212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StackingEnsemble(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, base_models, meta_learner):\n",
    "\n",
    "        self.base_models = base_models\n",
    "        self.meta_learner = meta_learner\n",
    "        self.fitted_base_models = None\n",
    "        self.fitted_meta_learner = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.fitted_base_models = {}\n",
    "\n",
    "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, (name, model) in enumerate(self.base_models.items()):\n",
    "            print(f\"Training base model: {name}\")\n",
    "\n",
    "            model.fit(X, y)\n",
    "\n",
    "            self.fitted_base_models[name] = model\n",
    "\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                meta_features[:, i] = model.predict(X)\n",
    "\n",
    "        print(\"Training meta-learner\")\n",
    "        self.fitted_meta_learner = self.meta_learner.fit(meta_features, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, (name, model) in enumerate(self.fitted_base_models.items()):\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                meta_features[:, i] = model.predict(X)\n",
    "\n",
    "        return self.fitted_meta_learner.predict(meta_features)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "\n",
    "        for i, (name, model) in enumerate(self.fitted_base_models.items()):\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                meta_features[:, i] = model.predict(X)\n",
    "\n",
    "        return self.fitted_meta_learner.predict_proba(meta_features)"
   ],
   "id": "5794ba78ecccef0f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T08:42:45.603101Z",
     "start_time": "2025-03-10T08:42:45.564243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_advanced_ensemble(X_train, y_train, X_val, y_val, X_test):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import xgboost as xgb\n",
    "    import catboost as cb\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "    import optuna\n",
    "    from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Replace GridSearchCV with Optuna-based optimization\n",
    "    base_models = tune_base_models_with_optuna(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        n_trials=50,  # Adjust based on your time constraints\n",
    "        timeout=1800,  # 30 minutes per model max\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    expected_models = ['logistic', 'xgboost', 'catboost']\n",
    "    for model_name in expected_models:\n",
    "        if model_name not in base_models:\n",
    "            print(f\"Warning: {model_name} is missing from base_models!\")\n",
    "\n",
    "    meta_features_train = np.zeros((X_train.shape[0], len(base_models)))\n",
    "    meta_features_val = np.zeros((X_val.shape[0], len(base_models)))\n",
    "    meta_features_test = np.zeros((X_test.shape[0], len(base_models)))\n",
    "\n",
    "    used_models = []\n",
    "\n",
    "    i = 0\n",
    "    for name, model in base_models.items():\n",
    "        print(f\"Generating meta-features from {name}...\")\n",
    "        used_models.append(name)\n",
    "\n",
    "        try:\n",
    "            _ = model.predict(X_train[:1])\n",
    "        except:\n",
    "            print(f\"Model {name} wasn't fitted, fitting now...\")\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            meta_features_train[:, i] = model.predict_proba(X_train)[:, 1]\n",
    "            meta_features_val[:, i] = model.predict_proba(X_val)[:, 1]\n",
    "            meta_features_test[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            meta_features_train[:, i] = model.predict(X_train)\n",
    "            meta_features_val[:, i] = model.predict(X_val)\n",
    "            meta_features_test[:, i] = model.predict(X_test)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(f\"Generated meta-features from {len(used_models)} models: {', '.join(used_models)}\")\n",
    "\n",
    "    # Now let's also optimize the meta-learners with Optuna!\n",
    "    def optimize_meta_learner(model_type, X, y, cv=5):\n",
    "        \"\"\"Optimize meta-learner hyperparameters with Optuna\"\"\"\n",
    "        cv_object = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "        if model_type == 'logistic':\n",
    "            def objective(trial):\n",
    "                C = trial.suggest_float('C', 0.001, 10.0, log=True)\n",
    "                model = LogisticRegression(C=C, max_iter=2000, random_state=42)\n",
    "                scores = cross_val_score(model, X, y, cv=cv_object, scoring='roc_auc')\n",
    "                return scores.mean()\n",
    "\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=30)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            print(f\"Best logistic meta-learner params: {best_params}\")\n",
    "            return LogisticRegression(C=best_params['C'], max_iter=2000, random_state=42)\n",
    "\n",
    "        elif model_type == 'xgb':\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 2, 5),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "                }\n",
    "                model = xgb.XGBClassifier(**params, random_state=42)\n",
    "                scores = cross_val_score(model, X, y, cv=cv_object, scoring='roc_auc')\n",
    "                return scores.mean()\n",
    "\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=30)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            print(f\"Best XGBoost meta-learner params: {best_params}\")\n",
    "            return xgb.XGBClassifier(**best_params, random_state=42)\n",
    "\n",
    "        elif model_type == 'catboost':\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'iterations': trial.suggest_int('iterations', 50, 200),\n",
    "                    'depth': trial.suggest_int('depth', 2, 5),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "                }\n",
    "                model = cb.CatBoostClassifier(**params, random_state=42, verbose=0)\n",
    "                scores = cross_val_score(model, X, y, cv=cv_object, scoring='roc_auc')\n",
    "                return scores.mean()\n",
    "\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=30)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            print(f\"Best CatBoost meta-learner params: {best_params}\")\n",
    "            return cb.CatBoostClassifier(**best_params, random_state=42, verbose=0)\n",
    "\n",
    "    meta_learners = {}\n",
    "    for model_type in ['logistic', 'xgb', 'catboost']:\n",
    "        print(f\"Optimizing meta-learner: {model_type}...\")\n",
    "        optimized_model = optimize_meta_learner(model_type, meta_features_train, y_train)\n",
    "        optimized_model.fit(meta_features_train, y_train)\n",
    "        meta_learners[model_type] = optimized_model\n",
    "\n",
    "    meta_preds_val = {}\n",
    "    meta_preds_test = {}\n",
    "\n",
    "    for name, model in meta_learners.items():\n",
    "        print(f\"Evaluating meta-learner: {name}...\")\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            val_pred = model.predict_proba(meta_features_val)[:, 1]\n",
    "        else:\n",
    "            val_pred = model.predict(meta_features_val)\n",
    "\n",
    "        acc = accuracy_score(y_val, (val_pred > 0.5).astype(int))\n",
    "        auc = roc_auc_score(y_val, val_pred)\n",
    "        print(f\"{name} - Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "        meta_preds_val[name] = val_pred\n",
    "        meta_preds_test[name] = model.predict_proba(meta_features_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(meta_features_test)\n",
    "\n",
    "    print(\"\\nOptimizing ensemble weights...\")\n",
    "\n",
    "    def objective(trial):\n",
    "        raw_weights = [\n",
    "            trial.suggest_float(f'weight_{name}', 0.1, 1.0)\n",
    "            for name in meta_preds_val.keys()\n",
    "        ]\n",
    "\n",
    "        weights_sum = sum(raw_weights)\n",
    "        normalized_weights = [w / weights_sum for w in raw_weights]\n",
    "\n",
    "        weighted_preds = np.zeros(len(X_val))\n",
    "        for i, (name, pred) in enumerate(meta_preds_val.items()):\n",
    "            weighted_preds += normalized_weights[i] * pred\n",
    "\n",
    "        return roc_auc_score(y_val, weighted_preds)\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    raw_weights = [\n",
    "        study.best_params[f'weight_{name}']\n",
    "        for name in meta_preds_val.keys()\n",
    "    ]\n",
    "    weights_sum = sum(raw_weights)\n",
    "    weights = {\n",
    "        name: raw_weights[i] / weights_sum\n",
    "        for i, name in enumerate(meta_preds_val.keys())\n",
    "    }\n",
    "\n",
    "    print(\"\\nOptimized meta-learner weights:\")\n",
    "    for name, weight in weights.items():\n",
    "        print(f\"{name}: {weight:.4f}\")\n",
    "\n",
    "    final_preds = np.zeros(len(X_test))\n",
    "    for i, (name, pred) in enumerate(meta_preds_test.items()):\n",
    "        final_preds += weights[name] * pred\n",
    "\n",
    "    final_val_preds = np.zeros(len(X_val))\n",
    "    for i, (name, pred) in enumerate(meta_preds_val.items()):\n",
    "        final_val_preds += weights[name] * pred\n",
    "\n",
    "    default_threshold = 0.5\n",
    "    default_val_preds = (final_val_preds > default_threshold).astype(int)\n",
    "    default_acc = accuracy_score(y_val, default_val_preds)\n",
    "    ensemble_auc = roc_auc_score(y_val, final_val_preds)\n",
    "\n",
    "    print(\"\\n===== DEFAULT THRESHOLD EVALUATION =====\")\n",
    "    print(f\"Default Threshold: {default_threshold}\")\n",
    "    print(f\"Validation Accuracy: {default_acc:.4f}\")\n",
    "    print(f\"Validation AUC: {ensemble_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nDefault Classification Report:\")\n",
    "    print(classification_report(y_val, default_val_preds))\n",
    "\n",
    "    print(\"\\n===== THRESHOLD OPTIMIZATION WITH OPTUNA =====\")\n",
    "\n",
    "    def threshold_objective(trial):\n",
    "        threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
    "        preds = (final_val_preds > threshold).astype(int)\n",
    "        return accuracy_score(y_val, preds)\n",
    "\n",
    "    threshold_study = optuna.create_study(direction='maximize')\n",
    "    threshold_study.optimize(threshold_objective, n_trials=100)\n",
    "\n",
    "    optimal_threshold = threshold_study.best_params['threshold']\n",
    "    optimal_acc = threshold_study.best_value\n",
    "\n",
    "    optimal_val_preds = (final_val_preds > optimal_threshold).astype(int)\n",
    "\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"Optimal Validation Accuracy: {optimal_acc:.4f}\")\n",
    "    print(f\"Improvement: {(optimal_acc - default_acc) * 100:.4f}%\")\n",
    "\n",
    "    print(\"\\nOptimized Classification Report:\")\n",
    "    print(classification_report(y_val, optimal_val_preds))\n",
    "\n",
    "    optimal_test_preds = (final_preds > optimal_threshold).astype(int)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nTotal ensemble building time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "\n",
    "    return {\n",
    "        'base_models': base_models,\n",
    "        'meta_learners': meta_learners,\n",
    "        'weights': weights,\n",
    "        'final_predictions': final_preds,\n",
    "        'final_binary_predictions': {\n",
    "            'default': (final_preds > default_threshold).astype(int),\n",
    "            'optimized': optimal_test_preds\n",
    "        },\n",
    "        'validation_predictions': final_val_preds,\n",
    "        'validation_binary_predictions': {\n",
    "            'default': default_val_preds,\n",
    "            'optimized': optimal_val_preds\n",
    "        },\n",
    "        'thresholds': {\n",
    "            'default': default_threshold,\n",
    "            'optimal': optimal_threshold\n",
    "        },\n",
    "        'meta_features': {\n",
    "            'train': meta_features_train,\n",
    "            'val': meta_features_val,\n",
    "            'test': meta_features_test\n",
    "        },\n",
    "        'performance': {\n",
    "            'default_accuracy': default_acc,\n",
    "            'optimal_accuracy': optimal_acc,\n",
    "            'improvement': optimal_acc - default_acc,\n",
    "            'auc': ensemble_auc\n",
    "        },\n",
    "        'optuna_studies': {\n",
    "            'base_models': None,\n",
    "            'meta_weights': study,\n",
    "            'threshold': threshold_study\n",
    "        }\n",
    "    }"
   ],
   "id": "d9022be9fa0e1ec3",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T14:13:40.815180Z",
     "start_time": "2025-03-09T14:13:40.806471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_nan_values(data_dict):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for key in ['X_train', 'X_val', 'X_test']:\n",
    "        if key in data_dict and data_dict[key] is not None:\n",
    "            df = data_dict[key]\n",
    "\n",
    "            has_nan = df.isna().any().any()\n",
    "\n",
    "            if has_nan:\n",
    "                nan_cols = df.columns[df.isna().any()].tolist()\n",
    "                nan_counts = {col: df[col].isna().sum() for col in nan_cols}\n",
    "                nan_percent = {col: (count / len(df) * 100) for col, count in nan_counts.items()}\n",
    "\n",
    "                results[key] = {\n",
    "                    'has_nan': True,\n",
    "                    'nan_columns': nan_cols,\n",
    "                    'nan_counts': nan_counts,\n",
    "                    'nan_percentages': nan_percent\n",
    "                }\n",
    "\n",
    "                print(f\"\\n{key} contains NaN values:\")\n",
    "                for col in nan_cols:\n",
    "                    print(f\"  - {col}: {nan_counts[col]} NaNs ({nan_percent[col]:.2f}%)\")\n",
    "            else:\n",
    "                results[key] = {'has_nan': False}\n",
    "                print(f\"\\n{key} does not contain any NaN values.\")\n",
    "\n",
    "    return results"
   ],
   "id": "9bb6bb3c44c822a2",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T09:18:56.765693Z",
     "start_time": "2025-03-10T09:18:56.694138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tune_base_models(X_train, y_train, cv=5, use_stratified_cv=True):\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    print(\"Tuning model hyperparameters...\")\n",
    "\n",
    "    param_grids = {\n",
    "        'logistic': {\n",
    "            'C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'solver': ['liblinear'],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        },\n",
    "        # 'random_forest': {\n",
    "        #     'n_estimators': [100, 200],\n",
    "        #     'max_depth': [8, 10, 12, 15],\n",
    "        #     'min_samples_split': [2, 5, 10],\n",
    "        #     'min_samples_leaf': [1, 2, 4]\n",
    "        # },\n",
    "        'xgboost': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        },\n",
    "        'catboost': {\n",
    "            'iterations': [100, 200],\n",
    "            'depth': [5, 6, 7],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'l2_leaf_reg': [1, 3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    base_estimators = {\n",
    "        'logistic': LogisticRegression(random_state=42, max_iter=2000),\n",
    "        #'random_forest': RandomForestClassifier(random_state=42),\n",
    "        'xgboost': xgb.XGBClassifier(random_state=42),\n",
    "        'catboost': cb.CatBoostClassifier(random_state=42, verbose=0)\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "\n",
    "    if use_stratified_cv:\n",
    "        stratified_cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "        print(f\"Using {cv}-fold StratifiedKFold cross-validation for hyperparameter tuning\")\n",
    "        cv_object = stratified_cv\n",
    "    else:\n",
    "        cv_object = cv\n",
    "\n",
    "    for name, estimator in base_estimators.items():\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid=param_grids[name],\n",
    "            cv=cv_object,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=os.cpu_count()-1,\n",
    "            verbose=2,\n",
    "            return_train_score=True\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Grid search completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "        best_index = grid_search.best_index_\n",
    "        if hasattr(grid_search, 'cv_results_') and 'mean_train_score' in grid_search.cv_results_:\n",
    "            train_score = grid_search.cv_results_['mean_train_score'][best_index]\n",
    "            cv_score = grid_search.best_score_\n",
    "            overfit_gap = train_score - cv_score\n",
    "            print(f\"Train score: {train_score:.4f}\")\n",
    "            print(f\"CV score: {cv_score:.4f}\")\n",
    "            print(f\"Potential overfitting: {overfit_gap:.4f}\")\n",
    "\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "\n",
    "        print(f\"Best {name} parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    return best_models\n",
    "def tune_base_models_with_optuna(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    n_trials=200,\n",
    "    timeout=None,\n",
    "    random_state=42\n",
    "):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Tuning model hyperparameters with Optuna...\")\n",
    "    print(f\"Using {cv}-fold StratifiedKFold cross-validation\")\n",
    "\n",
    "    best_models = {}\n",
    "\n",
    "\n",
    "    cv_object = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "\n",
    "    def optimize_logistic(trial):\n",
    "        C = trial.suggest_float('C', 0.001, 100, log=True)\n",
    "        penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "        solver = 'liblinear'\n",
    "\n",
    "        model = LogisticRegression(\n",
    "            C=C,\n",
    "            penalty=penalty,\n",
    "            solver=solver,\n",
    "            random_state=random_state,\n",
    "            max_iter=2000\n",
    "        )\n",
    "\n",
    "        scores = cross_val_score(\n",
    "            model, X_train, y_train,\n",
    "            cv=cv_object,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=os.cpu_count()-1\n",
    "        )\n",
    "\n",
    "        return scores.mean()\n",
    "\n",
    "    def optimize_xgboost(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)\n",
    "        }\n",
    "\n",
    "        model = xgb.XGBClassifier(\n",
    "            **params,\n",
    "            random_state=random_state,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "\n",
    "        scores = cross_val_score(\n",
    "            model, X_train, y_train,\n",
    "            cv=cv_object,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=os.cpu_count()-1\n",
    "        )\n",
    "\n",
    "        return scores.mean()\n",
    "\n",
    "    def optimize_catboost(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0, log=True),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 10.0),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1e-9, 10.0, log=True),\n",
    "            'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "            'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide'])\n",
    "        }\n",
    "\n",
    "        model = cb.CatBoostClassifier(\n",
    "            **params,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        scores = cross_val_score(\n",
    "            model, X_train, y_train,\n",
    "            cv=cv_object,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=os.cpu_count()-1\n",
    "        )\n",
    "\n",
    "        return scores.mean()\n",
    "\n",
    "\n",
    "    optimization_funcs = {\n",
    "        'logistic': optimize_logistic,\n",
    "        'xgboost': optimize_xgboost,\n",
    "        'catboost': optimize_catboost\n",
    "    }\n",
    "\n",
    "\n",
    "    for model_name, objective_func in optimization_funcs.items():\n",
    "        print(f\"\\nTuning {model_name}...\")\n",
    "        model_start_time = time.time()\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective_func, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "        model_elapsed = time.time() - model_start_time\n",
    "        print(f\"Optimization completed in {model_elapsed:.2f} seconds ({model_elapsed/60:.2f} minutes)\")\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_score = study.best_value\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best CV score: {best_score:.4f}\")\n",
    "\n",
    "        if model_name == 'logistic':\n",
    "            best_model = LogisticRegression(\n",
    "                C=best_params['C'],\n",
    "                penalty=best_params['penalty'],\n",
    "                solver='liblinear',\n",
    "                random_state=random_state,\n",
    "                max_iter=2000\n",
    "            )\n",
    "        elif model_name == 'xgboost':\n",
    "            best_model = xgb.XGBClassifier(\n",
    "                **best_params,\n",
    "                random_state=random_state,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "        elif model_name == 'catboost':\n",
    "            best_model = cb.CatBoostClassifier(\n",
    "                **best_params,\n",
    "                random_state=random_state,\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            train_preds = best_model.predict_proba(X_train)[:, 1]\n",
    "        else:\n",
    "            train_preds = best_model.predict(X_train)\n",
    "\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        train_score = roc_auc_score(y_train, train_preds)\n",
    "        cv_score = best_score\n",
    "        overfit_gap = train_score - cv_score\n",
    "\n",
    "        print(f\"Train score: {train_score:.4f}\")\n",
    "        print(f\"CV score: {cv_score:.4f}\")\n",
    "        print(f\"Potential overfitting: {overfit_gap:.4f}\")\n",
    "\n",
    "        best_models[model_name] = best_model\n",
    "\n",
    "        print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "        print(f\"Best {model_name} parameters: {best_params}\")\n",
    "        print(f\"Best CV score: {best_score:.4f}\")\n",
    "\n",
    "\n",
    "        optuna.visualization.plot_optimization_history(study)\n",
    "        plt.title(f\"{model_name} Optimization History\")\n",
    "        plt.show()\n",
    "\n",
    "    total_elapsed = time.time() - start_time\n",
    "    print(f\"\\nTotal hyperparameter tuning time: {total_elapsed:.2f} seconds ({total_elapsed/60:.2f} minutes)\")\n",
    "\n",
    "    return best_models"
   ],
   "id": "5d201770499dd0ce",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T06:43:18.395839Z",
     "start_time": "2025-03-10T06:43:18.380243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select_best_features(X_train, y_train, X_val, X_test):\n",
    "\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    import xgboost as xgb\n",
    "\n",
    "    print(\"Performing feature selection...\")\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    selection = SelectFromModel(model, threshold='median', prefit=True).set_output(transform=\"pandas\")\n",
    "\n",
    "    X_train_selected = selection.transform(X_train)\n",
    "    X_val_selected = selection.transform(X_val)\n",
    "    X_test_selected = selection.transform(X_test)\n",
    "\n",
    "    print(f\"Selected {X_train_selected.shape[1]} features out of {X_train.shape[1]}\")\n",
    "\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    })\n",
    "    top_features = feature_importance.sort_values('Importance', ascending=False).head(10)\n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(top_features)\n",
    "\n",
    "    return X_train_selected, X_val_selected, X_test_selected"
   ],
   "id": "8ab1d1f373d936f6",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T06:43:54.643496Z",
     "start_time": "2025-03-10T06:43:54.626645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_threshold(y_true, y_prob, metric='accuracy', thresholds=None):\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    import numpy as np\n",
    "\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.3, 0.7, 0.01)\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_score = 0.0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob > threshold).astype(int)\n",
    "\n",
    "        if metric == 'accuracy':\n",
    "            score = accuracy_score(y_true, y_pred)\n",
    "        elif metric == 'f1':\n",
    "            score = f1_score(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_true, y_pred)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, y_pred)\n",
    "        else:\n",
    "            score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_score\n"
   ],
   "id": "e36a7da10e3afd60",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T09:39:30.603376Z",
     "start_time": "2025-03-10T09:19:08.590765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count())\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(os.cpu_count())\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(os.cpu_count())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def run_spaceship_titanic_solution():\n",
    "    print(\"===== DATA PREPARATION =====\")\n",
    "    data_dict = prepare_spaceship_data(train_data, test_data)\n",
    "\n",
    "    print(\"\\n===== FEATURE SELECTION =====\")\n",
    "    X_train_selected, X_val_selected, X_test_selected = select_best_features(\n",
    "        data_dict['X_train'],\n",
    "        data_dict['y_train'],\n",
    "        data_dict['X_val'],\n",
    "        data_dict['X_test']\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== BUILDING ADVANCED ENSEMBLE =====\")\n",
    "    ensemble_results = build_advanced_ensemble(\n",
    "        X_train_selected,\n",
    "        data_dict['y_train'],\n",
    "        X_val_selected,\n",
    "        data_dict['y_val'],\n",
    "        X_test_selected\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== FINAL EVALUATION =====\")\n",
    "    optimal_threshold = ensemble_results['thresholds']['optimal']\n",
    "    optimal_accuracy = ensemble_results['performance']['optimal_accuracy']\n",
    "    default_accuracy = ensemble_results['performance']['default_accuracy']\n",
    "\n",
    "    print(f\"Default Threshold (0.5) Accuracy: {default_accuracy:.4f}\")\n",
    "    print(f\"Optimal Threshold ({optimal_threshold:.4f}) Accuracy: {optimal_accuracy:.4f}\")\n",
    "    print(f\"Improvement: {(optimal_accuracy - default_accuracy) * 100:.4f}%\")\n",
    "\n",
    "    optimized_val_preds = ensemble_results['validation_binary_predictions']['optimized']\n",
    "\n",
    "    print(\"\\nFinal Classification Report (Optimized):\")\n",
    "    print(classification_report(data_dict['y_val'], optimized_val_preds))\n",
    "\n",
    "    print(\"\\n===== GENERATING SUBMISSION =====\")\n",
    "    final_test_preds = ensemble_results['final_predictions']\n",
    "    final_test_labels = (final_test_preds > optimal_threshold).astype(bool)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'PassengerId': data_dict['test_ids'],\n",
    "        'Transported': final_test_labels\n",
    "    })\n",
    "\n",
    "    submission_path = 'submission.csv'\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission file created: {submission_path}\")\n",
    "    print(f\"Using optimal threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'data_dict': data_dict,\n",
    "        'ensemble_results': ensemble_results,\n",
    "        'final_metrics': {\n",
    "            'default_accuracy': default_accuracy,\n",
    "            'optimal_accuracy': optimal_accuracy,\n",
    "            'auc': ensemble_results['performance']['auc']\n",
    "        },\n",
    "        'submission': submission\n",
    "    }\n",
    "\n",
    "results = run_spaceship_titanic_solution()"
   ],
   "id": "c2a0c841f23ca62b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== DATA PREPARATION =====\n",
      "Loading and preparing Spaceship Titanic data...\n",
      "Training data: 8693 rows, 14 columns\n",
      "Test data: 4277 rows, 13 columns\n",
      "Converted boolean target to integer (0/1)\n",
      "Split into 6954 training samples and 1739 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_37360/4228832619.py:134: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0. 0. 0. ... 0. 0. 0.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_37360/4228832619.py:134: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.         0.2699005  0.99688958 ... 0.99654378 0.83202358 0.83671558]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_37360/4228832619.py:134: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.01152074 0.         0.         ... 0.         0.01669941 0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_37360/4228832619.py:134: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.00019201 0.09577114 0.00311042 ... 0.00345622 0.00982318 0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_37360/4228832619.py:134: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.98828725 0.63432836 0.         ... 0.         0.14145383 0.16328442]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_37360/4228832619.py:152: FutureWarning:\n",
      "\n",
      "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_37360/4228832619.py:152: FutureWarning:\n",
      "\n",
      "Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing: 5350 features created\n",
      "Found NaN values in X_train_proc for columns: ['CabinNum', 'FamilySize']\n",
      "Filling remaining NaNs with appropriate values...\n",
      "  - CabinNum: filled with median (429.0)\n",
      "  - FamilySize: filled with median (6.0)\n",
      "Found NaN values in X_val_proc for columns: ['CabinNum', 'FamilySize']\n",
      "Filling remaining NaNs with appropriate values...\n",
      "  - CabinNum: filled with median (420.0)\n",
      "  - FamilySize: filled with median (6.0)\n",
      "Found NaN values in X_test_proc for columns: ['CabinNum', 'FamilySize']\n",
      "Filling remaining NaNs with appropriate values...\n",
      "  - CabinNum: filled with median (442.0)\n",
      "  - FamilySize: filled with median (6.0)\n",
      "All datasets are now free of NaN values!\n",
      "Processed training features: (6954, 5350)\n",
      "Processed validation features: (1739, 5350)\n",
      "Processed test features: (4277, 5350)\n",
      "\n",
      "===== FEATURE SELECTION =====\n",
      "Performing feature selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2732: UserWarning:\n",
      "\n",
      "X has feature names, but SelectFromModel was fitted without feature names\n",
      "\n",
      "[I 2025-03-10 16:19:12,975] A new study created in memory with name: no-name-4951278d-90a8-49e0-9e8f-49087aeb2ecb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5350 features out of 5350\n",
      "Top 10 most important features:\n",
      "                        Feature  Importance\n",
      "0                 ActiveSpender    0.697710\n",
      "12                  CabinDeck_G    0.040102\n",
      "18                    CryoSleep    0.017624\n",
      "2430             FoodCourtRatio    0.015067\n",
      "10                  CabinDeck_E    0.014080\n",
      "5334          ShoppingMallRatio    0.012825\n",
      "2436            HomePlanet_Mars    0.008964\n",
      "16                  CabinSide_S    0.006110\n",
      "2435          HomePlanet_Europa    0.005724\n",
      "5331  Route_Mars_to_TRAPPIST-1e    0.005463\n",
      "\n",
      "===== BUILDING ADVANCED ENSEMBLE =====\n",
      "Tuning model hyperparameters with Optuna...\n",
      "Using 5-fold StratifiedKFold cross-validation\n",
      "\n",
      "Tuning logistic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:19:15,334] Trial 0 finished with value: 0.8776679432981893 and parameters: {'C': 0.03324785639616137, 'penalty': 'l2'}. Best is trial 0 with value: 0.8776679432981893.\n",
      "[I 2025-03-10 16:19:16,652] Trial 1 finished with value: 0.874790276872516 and parameters: {'C': 0.027956626824766368, 'penalty': 'l1'}. Best is trial 0 with value: 0.8776679432981893.\n",
      "[I 2025-03-10 16:19:18,002] Trial 2 finished with value: 0.8782037826063149 and parameters: {'C': 0.12722070144662892, 'penalty': 'l2'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:19,330] Trial 3 finished with value: 0.8747992206747511 and parameters: {'C': 0.008455934828511561, 'penalty': 'l2'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:20,539] Trial 4 finished with value: 0.8750747999204807 and parameters: {'C': 0.5972491356139578, 'penalty': 'l2'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:21,719] Trial 5 finished with value: 0.8781492899634795 and parameters: {'C': 0.30649687189030633, 'penalty': 'l1'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:22,880] Trial 6 finished with value: 0.8776307510333791 and parameters: {'C': 0.1917204898651115, 'penalty': 'l1'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:24,083] Trial 7 finished with value: 0.8748684376159288 and parameters: {'C': 0.028852513266643967, 'penalty': 'l1'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:25,551] Trial 8 finished with value: 0.8614973246049843 and parameters: {'C': 6.61105022768802, 'penalty': 'l2'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:26,807] Trial 9 finished with value: 0.8769567503859289 and parameters: {'C': 0.35722685875840227, 'penalty': 'l2'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:27,911] Trial 10 finished with value: 0.86267966995829 and parameters: {'C': 0.0015164755388540996, 'penalty': 'l2'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:29,451] Trial 11 finished with value: 0.8225841788121709 and parameters: {'C': 7.594608218263139, 'penalty': 'l1'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:31,388] Trial 12 finished with value: 0.7595508937676851 and parameters: {'C': 70.62686496746797, 'penalty': 'l1'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:32,645] Trial 13 finished with value: 0.8753716664531053 and parameters: {'C': 1.0524717080381383, 'penalty': 'l1'}. Best is trial 2 with value: 0.8782037826063149.\n",
      "[I 2025-03-10 16:19:33,879] Trial 14 finished with value: 0.8782136145494391 and parameters: {'C': 0.11735376611011784, 'penalty': 'l2'}. Best is trial 14 with value: 0.8782136145494391.\n",
      "[I 2025-03-10 16:19:35,039] Trial 15 finished with value: 0.8780281786919284 and parameters: {'C': 0.07833767965016146, 'penalty': 'l2'}. Best is trial 14 with value: 0.8782136145494391.\n",
      "[I 2025-03-10 16:19:36,242] Trial 16 finished with value: 0.8640118949916806 and parameters: {'C': 0.0017803503606459367, 'penalty': 'l2'}. Best is trial 14 with value: 0.8782136145494391.\n",
      "[I 2025-03-10 16:19:37,575] Trial 17 finished with value: 0.8644460503817705 and parameters: {'C': 2.213951314651092, 'penalty': 'l2'}. Best is trial 14 with value: 0.8782136145494391.\n",
      "[I 2025-03-10 16:19:38,827] Trial 18 finished with value: 0.8756194047890566 and parameters: {'C': 0.011783734280358735, 'penalty': 'l2'}. Best is trial 14 with value: 0.8782136145494391.\n",
      "[I 2025-03-10 16:19:40,027] Trial 19 finished with value: 0.8778308400021091 and parameters: {'C': 0.10367603019404868, 'penalty': 'l2'}. Best is trial 14 with value: 0.8782136145494391.\n",
      "[I 2025-03-10 16:19:41,573] Trial 20 finished with value: 0.8701539716184913 and parameters: {'C': 2.8714491427429856, 'penalty': 'l2'}. Best is trial 14 with value: 0.8782136145494391.\n",
      "[I 2025-03-10 16:19:42,795] Trial 21 finished with value: 0.8773478511971401 and parameters: {'C': 0.15308244913464106, 'penalty': 'l1'}. Best is trial 14 with value: 0.8782136145494391.\n",
      "[I 2025-03-10 16:19:44,056] Trial 22 finished with value: 0.8785519848997986 and parameters: {'C': 0.6253863734301369, 'penalty': 'l1'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:45,364] Trial 23 finished with value: 0.8769283785625452 and parameters: {'C': 0.8996190243731349, 'penalty': 'l1'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:46,698] Trial 24 finished with value: 0.8591079403493198 and parameters: {'C': 19.336857668560718, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:47,774] Trial 25 finished with value: 0.8782360557781006 and parameters: {'C': 0.055503115555986596, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:48,811] Trial 26 finished with value: 0.8409531598767079 and parameters: {'C': 0.00504330870072876, 'penalty': 'l1'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:49,891] Trial 27 finished with value: 0.8780215442546705 and parameters: {'C': 0.03685073876161023, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:50,875] Trial 28 finished with value: 0.8761022683542704 and parameters: {'C': 0.05087681534592161, 'penalty': 'l1'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:51,886] Trial 29 finished with value: 0.8764886747485058 and parameters: {'C': 0.015070742256195245, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:53,122] Trial 30 finished with value: 0.8688293028620475 and parameters: {'C': 1.745350129541337, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:54,402] Trial 31 finished with value: 0.8768675425918371 and parameters: {'C': 0.2661521483846856, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:55,677] Trial 32 finished with value: 0.87821335823049 and parameters: {'C': 0.07956319045146823, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:56,834] Trial 33 finished with value: 0.8781650706188868 and parameters: {'C': 0.0626150382585063, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:57,980] Trial 34 finished with value: 0.8763830581665729 and parameters: {'C': 0.5139056143342007, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:19:59,110] Trial 35 finished with value: 0.8775188008063897 and parameters: {'C': 0.026213527475742245, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:00,528] Trial 36 finished with value: 0.8783626015870271 and parameters: {'C': 0.1215175297326342, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:01,772] Trial 37 finished with value: 0.8774123711525365 and parameters: {'C': 0.16132736445127416, 'penalty': 'l1'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:02,822] Trial 38 finished with value: 0.876561661864004 and parameters: {'C': 0.48521593589568573, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:03,706] Trial 39 finished with value: 0.8410317744721165 and parameters: {'C': 0.005501212350548828, 'penalty': 'l1'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:04,786] Trial 40 finished with value: 0.8777929098865099 and parameters: {'C': 0.26442784154119015, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:05,885] Trial 41 finished with value: 0.8780967238104573 and parameters: {'C': 0.09572907478181988, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:06,954] Trial 42 finished with value: 0.8773811727178135 and parameters: {'C': 0.029571209813317855, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:07,985] Trial 43 finished with value: 0.8773810558692056 and parameters: {'C': 0.020714457281074, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:09,091] Trial 44 finished with value: 0.8780463262988702 and parameters: {'C': 0.052400490802628916, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:10,287] Trial 45 finished with value: 0.8736147010995895 and parameters: {'C': 0.7560563214179198, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:11,452] Trial 46 finished with value: 0.8771468790156959 and parameters: {'C': 0.13096959174868442, 'penalty': 'l1'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:12,634] Trial 47 finished with value: 0.8773364297958001 and parameters: {'C': 0.3927737283033983, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:13,686] Trial 48 finished with value: 0.8780020311936545 and parameters: {'C': 0.22937820997104308, 'penalty': 'l2'}. Best is trial 22 with value: 0.8785519848997986.\n",
      "[I 2025-03-10 16:20:14,882] Trial 49 finished with value: 0.872231597493791 and parameters: {'C': 1.2870790972733557, 'penalty': 'l1'}. Best is trial 22 with value: 0.8785519848997986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed in 61.91 seconds (1.03 minutes)\n",
      "Best parameters: {'C': 0.6253863734301369, 'penalty': 'l1'}\n",
      "Best CV score: 0.8786\n",
      "Train score: 0.8945\n",
      "CV score: 0.8786\n",
      "Potential overfitting: 0.0159\n",
      "Number of finished trials: 50\n",
      "Best logistic parameters: {'C': 0.6253863734301369, 'penalty': 'l1'}\n",
      "Best CV score: 0.8786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:20:15,477] A new study created in memory with name: no-name-e1905832-1850-40ad-a153-d8191b30862d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning xgboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:20:32,094] Trial 0 finished with value: 0.8919711455822055 and parameters: {'n_estimators': 394, 'max_depth': 3, 'learning_rate': 0.09988188359251168, 'subsample': 0.6308252651274511, 'colsample_bytree': 0.7083672123927529, 'min_child_weight': 1, 'gamma': 0.5013811137918903, 'reg_alpha': 0.00019817784158016462, 'reg_lambda': 2.2158301469746623e-07}. Best is trial 0 with value: 0.8919711455822055.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:20:47,114] Trial 1 finished with value: 0.8908177292113052 and parameters: {'n_estimators': 179, 'max_depth': 7, 'learning_rate': 0.010171978120699551, 'subsample': 0.8856688720271588, 'colsample_bytree': 0.5561692818609338, 'min_child_weight': 1, 'gamma': 0.07867619457142716, 'reg_alpha': 5.744986929672377e-08, 'reg_lambda': 0.4491218787677311}. Best is trial 0 with value: 0.8919711455822055.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:20:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:21:01,636] Trial 2 finished with value: 0.8890137362732217 and parameters: {'n_estimators': 229, 'max_depth': 7, 'learning_rate': 0.1045492896862424, 'subsample': 0.7727595450111857, 'colsample_bytree': 0.5337983722739164, 'min_child_weight': 5, 'gamma': 0.29780140794486465, 'reg_alpha': 0.0013388307620974909, 'reg_lambda': 0.00017642156197843824}. Best is trial 0 with value: 0.8919711455822055.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:21:20,325] Trial 3 finished with value: 0.8925197251892394 and parameters: {'n_estimators': 377, 'max_depth': 6, 'learning_rate': 0.06246646925301525, 'subsample': 0.8965322393154178, 'colsample_bytree': 0.7087918707214229, 'min_child_weight': 5, 'gamma': 0.6372201986445291, 'reg_alpha': 9.64003847375667e-08, 'reg_lambda': 0.002817595589092524}. Best is trial 3 with value: 0.8925197251892394.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:21:39,612] Trial 4 finished with value: 0.8961588372705241 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.05468455962906509, 'subsample': 0.9184008026514097, 'colsample_bytree': 0.5242662304901133, 'min_child_weight': 1, 'gamma': 0.3584116165346153, 'reg_alpha': 5.9874833242423454e-05, 'reg_lambda': 0.0014719810529287094}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:21:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:21:58,055] Trial 5 finished with value: 0.8946860486989248 and parameters: {'n_estimators': 350, 'max_depth': 5, 'learning_rate': 0.013608479241786231, 'subsample': 0.8483129721822532, 'colsample_bytree': 0.7817760080869787, 'min_child_weight': 4, 'gamma': 0.3701177492363621, 'reg_alpha': 1.0875423508675003e-07, 'reg_lambda': 0.00034033368229435465}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:22:17,495] Trial 6 finished with value: 0.892215462411141 and parameters: {'n_estimators': 380, 'max_depth': 4, 'learning_rate': 0.01051516296753257, 'subsample': 0.8084179420315694, 'colsample_bytree': 0.9744150066111876, 'min_child_weight': 2, 'gamma': 0.15569937394663935, 'reg_alpha': 4.229419160485993e-06, 'reg_lambda': 0.008728040864658601}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:22:32,699] Trial 7 finished with value: 0.8939823786520389 and parameters: {'n_estimators': 381, 'max_depth': 3, 'learning_rate': 0.0627886236012965, 'subsample': 0.7227727246222966, 'colsample_bytree': 0.9078306650847592, 'min_child_weight': 9, 'gamma': 0.049792973018319864, 'reg_alpha': 0.05435047970414474, 'reg_lambda': 1.1750023904128054e-07}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:22:43,804] Trial 8 finished with value: 0.8937896332037223 and parameters: {'n_estimators': 181, 'max_depth': 5, 'learning_rate': 0.022632775243844563, 'subsample': 0.5762069818430497, 'colsample_bytree': 0.7259000188498566, 'min_child_weight': 6, 'gamma': 0.9662431193194806, 'reg_alpha': 5.4050229625222455e-08, 'reg_lambda': 3.116845611852277e-05}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:22:56,992] Trial 9 finished with value: 0.889505212710094 and parameters: {'n_estimators': 247, 'max_depth': 4, 'learning_rate': 0.012676520598562644, 'subsample': 0.5015246877339032, 'colsample_bytree': 0.6346397097170918, 'min_child_weight': 4, 'gamma': 0.06266949629451513, 'reg_alpha': 0.005415220254623457, 'reg_lambda': 8.438474264921748e-07}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:22:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:23:13,702] Trial 10 finished with value: 0.8891455384179163 and parameters: {'n_estimators': 490, 'max_depth': 6, 'learning_rate': 0.2414528602690888, 'subsample': 0.9924122004002582, 'colsample_bytree': 0.8395812608483872, 'min_child_weight': 10, 'gamma': 0.7225047505872031, 'reg_alpha': 9.56479886209431e-06, 'reg_lambda': 0.6970044628596078}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:23:20,409] Trial 11 finished with value: 0.8880852247794634 and parameters: {'n_estimators': 65, 'max_depth': 5, 'learning_rate': 0.03050368339096827, 'subsample': 0.9856068831149304, 'colsample_bytree': 0.8138599211425206, 'min_child_weight': 3, 'gamma': 0.34617521351020686, 'reg_alpha': 2.153399271708941e-06, 'reg_lambda': 5.552304483320206e-05}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:23:41,113] Trial 12 finished with value: 0.8948392887497294 and parameters: {'n_estimators': 494, 'max_depth': 4, 'learning_rate': 0.02791268812164666, 'subsample': 0.8835747778850157, 'colsample_bytree': 0.5909778445042797, 'min_child_weight': 8, 'gamma': 0.3567642478584241, 'reg_alpha': 3.0042422232169443e-05, 'reg_lambda': 0.002602093110046578}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:23:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:24:00,158] Trial 13 finished with value: 0.8954559181334496 and parameters: {'n_estimators': 461, 'max_depth': 4, 'learning_rate': 0.0311105779092757, 'subsample': 0.9251286954446014, 'colsample_bytree': 0.5000314936953483, 'min_child_weight': 7, 'gamma': 0.49960513729935807, 'reg_alpha': 5.0951030137454056e-05, 'reg_lambda': 0.020260206513119813}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:24:17,687] Trial 14 finished with value: 0.8947495969631294 and parameters: {'n_estimators': 451, 'max_depth': 3, 'learning_rate': 0.04293996342122565, 'subsample': 0.9432970001693697, 'colsample_bytree': 0.5117683147930859, 'min_child_weight': 7, 'gamma': 0.614850281359577, 'reg_alpha': 0.639335432389766, 'reg_lambda': 0.03993088527265426}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:24:40,097] Trial 15 finished with value: 0.8866465740941711 and parameters: {'n_estimators': 443, 'max_depth': 8, 'learning_rate': 0.1077591792417521, 'subsample': 0.7107998809560917, 'colsample_bytree': 0.6362809216321312, 'min_child_weight': 7, 'gamma': 0.8083718658616595, 'reg_alpha': 0.000308397667808135, 'reg_lambda': 3.907114775851004e-06}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:24:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:24:58,090] Trial 16 finished with value: 0.8946111838128749 and parameters: {'n_estimators': 441, 'max_depth': 4, 'learning_rate': 0.041482029548434024, 'subsample': 0.9410842765787648, 'colsample_bytree': 0.6276760503272563, 'min_child_weight': 7, 'gamma': 0.49000774251956986, 'reg_alpha': 7.554892592814262e-07, 'reg_lambda': 1.1570614739670025e-08}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:25:13,154] Trial 17 finished with value: 0.8938521404390676 and parameters: {'n_estimators': 304, 'max_depth': 4, 'learning_rate': 0.018250229867450434, 'subsample': 0.8228582894704103, 'colsample_bytree': 0.5016574729146286, 'min_child_weight': 3, 'gamma': 0.25028774799322245, 'reg_alpha': 4.633567709946255e-05, 'reg_lambda': 0.05623825576056425}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:25:26,184] Trial 18 finished with value: 0.8891174660754035 and parameters: {'n_estimators': 297, 'max_depth': 3, 'learning_rate': 0.2339133583137574, 'subsample': 0.6739007103087078, 'colsample_bytree': 0.5761153177052405, 'min_child_weight': 9, 'gamma': 0.49302097028489633, 'reg_alpha': 0.0065304329162537865, 'reg_lambda': 0.0010940994419707058}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:25:47,276] Trial 19 finished with value: 0.8849521390392605 and parameters: {'n_estimators': 491, 'max_depth': 5, 'learning_rate': 0.16391060047520473, 'subsample': 0.9345738027442482, 'colsample_bytree': 0.6645789515445237, 'min_child_weight': 6, 'gamma': 0.20492171477027843, 'reg_alpha': 0.000907840741190438, 'reg_lambda': 0.06144606723239998}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:25:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:26:08,507] Trial 20 finished with value: 0.8933847711035657 and parameters: {'n_estimators': 422, 'max_depth': 6, 'learning_rate': 0.04661608416191047, 'subsample': 0.7866860733333394, 'colsample_bytree': 0.585713340457423, 'min_child_weight': 3, 'gamma': 0.42360067403929436, 'reg_alpha': 8.844662036675286e-07, 'reg_lambda': 0.01523791796907801}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:26:29,048] Trial 21 finished with value: 0.8952543658241835 and parameters: {'n_estimators': 498, 'max_depth': 4, 'learning_rate': 0.0289392174771623, 'subsample': 0.8716666159058933, 'colsample_bytree': 0.593184426606828, 'min_child_weight': 8, 'gamma': 0.44148207753526975, 'reg_alpha': 2.9505647377573853e-05, 'reg_lambda': 0.001658194606565928}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:26:48,391] Trial 22 finished with value: 0.8959364505539738 and parameters: {'n_estimators': 467, 'max_depth': 4, 'learning_rate': 0.033739487691476264, 'subsample': 0.8462798017895947, 'colsample_bytree': 0.5514611127965868, 'min_child_weight': 8, 'gamma': 0.5902891013429884, 'reg_alpha': 2.0496540437777685e-05, 'reg_lambda': 0.0005918432474794314}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:26:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:27:07,882] Trial 23 finished with value: 0.893167960105702 and parameters: {'n_estimators': 455, 'max_depth': 3, 'learning_rate': 0.07147280211284761, 'subsample': 0.9207480665195785, 'colsample_bytree': 0.5434080620072516, 'min_child_weight': 8, 'gamma': 0.6310227770725415, 'reg_alpha': 0.00014925200626889396, 'reg_lambda': 1.790636478722422e-05}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:27:24,820] Trial 24 finished with value: 0.8957585733088432 and parameters: {'n_estimators': 334, 'max_depth': 5, 'learning_rate': 0.03591512797203592, 'subsample': 0.842943587039457, 'colsample_bytree': 0.5141189841731686, 'min_child_weight': 10, 'gamma': 0.5568068025157393, 'reg_alpha': 1.2580205223246932e-05, 'reg_lambda': 0.0003529875232392549}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:27:42,378] Trial 25 finished with value: 0.8952779053075174 and parameters: {'n_estimators': 336, 'max_depth': 5, 'learning_rate': 0.01934061859386008, 'subsample': 0.8308441922495596, 'colsample_bytree': 0.6615854736971715, 'min_child_weight': 10, 'gamma': 0.7560735736842503, 'reg_alpha': 7.63079923709258e-06, 'reg_lambda': 0.00042596442699975644}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:27:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:28:02,464] Trial 26 finished with value: 0.8952315956883607 and parameters: {'n_estimators': 415, 'max_depth': 5, 'learning_rate': 0.03696287227290843, 'subsample': 0.7560768351261663, 'colsample_bytree': 0.5520963945934193, 'min_child_weight': 9, 'gamma': 0.5697505833980835, 'reg_alpha': 8.888578372595672e-07, 'reg_lambda': 5.389348848616358e-06}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:28:19,336] Trial 27 finished with value: 0.8927434079085016 and parameters: {'n_estimators': 341, 'max_depth': 6, 'learning_rate': 0.08223229003187121, 'subsample': 0.8552869539945597, 'colsample_bytree': 0.6155846521298174, 'min_child_weight': 10, 'gamma': 0.8787127484597093, 'reg_alpha': 1.2545892786688956e-08, 'reg_lambda': 0.00017723612762017161}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:28:25,741] Trial 28 finished with value: 0.8889326650151167 and parameters: {'n_estimators': 60, 'max_depth': 4, 'learning_rate': 0.05195039424272978, 'subsample': 0.9706267197786299, 'colsample_bytree': 0.5371017693206972, 'min_child_weight': 9, 'gamma': 0.7034447770285934, 'reg_alpha': 1.282268480769106e-05, 'reg_lambda': 0.0008510061822354795}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:28:41,527] Trial 29 finished with value: 0.8929154887129771 and parameters: {'n_estimators': 409, 'max_depth': 3, 'learning_rate': 0.12475970065068989, 'subsample': 0.7941331892528495, 'colsample_bytree': 0.6805683551969276, 'min_child_weight': 1, 'gamma': 0.5657950982589971, 'reg_alpha': 0.00034375094217145433, 'reg_lambda': 0.005029825759388002}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:28:51,379] Trial 30 finished with value: 0.8919614732888668 and parameters: {'n_estimators': 106, 'max_depth': 7, 'learning_rate': 0.021842501805923772, 'subsample': 0.7082937066524736, 'colsample_bytree': 0.5730659266487446, 'min_child_weight': 10, 'gamma': 0.41966833190007824, 'reg_alpha': 0.0018932359285159406, 'reg_lambda': 0.20335310753613994}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:28:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:29:11,622] Trial 31 finished with value: 0.895269498941774 and parameters: {'n_estimators': 475, 'max_depth': 4, 'learning_rate': 0.035548531144700114, 'subsample': 0.9053017402786617, 'colsample_bytree': 0.5128883163663644, 'min_child_weight': 8, 'gamma': 0.5833831226082504, 'reg_alpha': 8.278060636467969e-05, 'reg_lambda': 0.013997434165488262}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:29:30,263] Trial 32 finished with value: 0.8943972577839763 and parameters: {'n_estimators': 468, 'max_depth': 4, 'learning_rate': 0.05344807442689189, 'subsample': 0.9634047142752655, 'colsample_bytree': 0.5024567055215886, 'min_child_weight': 6, 'gamma': 0.4864687303210501, 'reg_alpha': 0.00010879932576405072, 'reg_lambda': 8.500841496694371e-05}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:29:50,079] Trial 33 finished with value: 0.8961057570755223 and parameters: {'n_estimators': 428, 'max_depth': 5, 'learning_rate': 0.03185731763088053, 'subsample': 0.8608036353930922, 'colsample_bytree': 0.5460789588898407, 'min_child_weight': 7, 'gamma': 0.665681349517423, 'reg_alpha': 2.322071657065299e-05, 'reg_lambda': 0.17922427838409646}. Best is trial 4 with value: 0.8961588372705241.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:29:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:30:10,127] Trial 34 finished with value: 0.8962669389267444 and parameters: {'n_estimators': 423, 'max_depth': 5, 'learning_rate': 0.024512219201275713, 'subsample': 0.8400604264576631, 'colsample_bytree': 0.553499338686353, 'min_child_weight': 8, 'gamma': 0.6845910536224237, 'reg_alpha': 3.34735001020093e-06, 'reg_lambda': 1.1255746551327947e-05}. Best is trial 34 with value: 0.8962669389267444.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:30:30,830] Trial 35 finished with value: 0.8948987177294997 and parameters: {'n_estimators': 425, 'max_depth': 6, 'learning_rate': 0.025470990573772397, 'subsample': 0.8762437099507007, 'colsample_bytree': 0.5532820507862314, 'min_child_weight': 5, 'gamma': 0.6791117663687098, 'reg_alpha': 1.5365561732160616e-06, 'reg_lambda': 9.283027538341693e-06}. Best is trial 34 with value: 0.8962669389267444.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:30:50,299] Trial 36 finished with value: 0.8959612786015958 and parameters: {'n_estimators': 402, 'max_depth': 5, 'learning_rate': 0.01661761015385887, 'subsample': 0.7558992411385628, 'colsample_bytree': 0.6132655978298182, 'min_child_weight': 1, 'gamma': 0.8696562725759422, 'reg_alpha': 2.6638979880583935e-07, 'reg_lambda': 2.499399692299444e-06}. Best is trial 34 with value: 0.8962669389267444.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:31:09,501] Trial 37 finished with value: 0.8959435197554182 and parameters: {'n_estimators': 366, 'max_depth': 5, 'learning_rate': 0.015433145429898648, 'subsample': 0.7353439474858156, 'colsample_bytree': 0.7070282969154089, 'min_child_weight': 1, 'gamma': 0.9053083486943764, 'reg_alpha': 2.7952428278012404e-07, 'reg_lambda': 1.3374909018107878e-06}. Best is trial 34 with value: 0.8962669389267444.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:31:33,373] Trial 38 finished with value: 0.8948631074789251 and parameters: {'n_estimators': 408, 'max_depth': 7, 'learning_rate': 0.01594837000007709, 'subsample': 0.6164337521342835, 'colsample_bytree': 0.6146937013842936, 'min_child_weight': 2, 'gamma': 0.8112670709592705, 'reg_alpha': 2.7699876486390576e-07, 'reg_lambda': 1.457067431573344e-07}. Best is trial 34 with value: 0.8962669389267444.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:31:54,939] Trial 39 finished with value: 0.8952593899364866 and parameters: {'n_estimators': 381, 'max_depth': 6, 'learning_rate': 0.011548917807516747, 'subsample': 0.7594419124501568, 'colsample_bytree': 0.7793368726347862, 'min_child_weight': 2, 'gamma': 0.9822660900729521, 'reg_alpha': 3.504535319162189e-06, 'reg_lambda': 8.059948860322861e-07}. Best is trial 34 with value: 0.8962669389267444.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:31:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:32:14,484] Trial 40 finished with value: 0.8966148563982198 and parameters: {'n_estimators': 395, 'max_depth': 5, 'learning_rate': 0.023828685908299667, 'subsample': 0.8084269581394411, 'colsample_bytree': 0.6032129756556399, 'min_child_weight': 1, 'gamma': 0.7755307904858088, 'reg_alpha': 2.806584158608301e-08, 'reg_lambda': 1.896833061924723e-06}. Best is trial 40 with value: 0.8966148563982198.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:32:32,023] Trial 41 finished with value: 0.8965545546023461 and parameters: {'n_estimators': 398, 'max_depth': 5, 'learning_rate': 0.022623778517709598, 'subsample': 0.811738717338682, 'colsample_bytree': 0.5729699022239919, 'min_child_weight': 1, 'gamma': 0.7879455254462454, 'reg_alpha': 1.4217709903877345e-08, 'reg_lambda': 4.072355845857479e-07}. Best is trial 40 with value: 0.8966148563982198.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:32:49,196] Trial 42 finished with value: 0.8956675553016554 and parameters: {'n_estimators': 369, 'max_depth': 5, 'learning_rate': 0.02063342718787848, 'subsample': 0.8086422682373958, 'colsample_bytree': 0.5631804916528719, 'min_child_weight': 2, 'gamma': 0.7758146363635174, 'reg_alpha': 1.324444114910115e-08, 'reg_lambda': 2.7124316066220546e-08}. Best is trial 40 with value: 0.8966148563982198.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:33:07,533] Trial 43 finished with value: 0.8966277681104197 and parameters: {'n_estimators': 436, 'max_depth': 5, 'learning_rate': 0.024446104851908713, 'subsample': 0.8971409273233597, 'colsample_bytree': 0.9781153049670848, 'min_child_weight': 1, 'gamma': 0.6551434308887464, 'reg_alpha': 2.272876998871788e-08, 'reg_lambda': 2.1862831593608459e-07}. Best is trial 43 with value: 0.8966277681104197.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:33:20,573] Trial 44 finished with value: 0.895366692553476 and parameters: {'n_estimators': 240, 'max_depth': 6, 'learning_rate': 0.024745325127351808, 'subsample': 0.8088344432227087, 'colsample_bytree': 0.9658102882402696, 'min_child_weight': 1, 'gamma': 0.745624408057407, 'reg_alpha': 3.624679639491208e-08, 'reg_lambda': 2.9526852755829093e-07}. Best is trial 43 with value: 0.8966277681104197.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:33:38,311] Trial 45 finished with value: 0.8953943388794192 and parameters: {'n_estimators': 388, 'max_depth': 5, 'learning_rate': 0.014079141626878993, 'subsample': 0.9023405933367434, 'colsample_bytree': 0.891469221322718, 'min_child_weight': 2, 'gamma': 0.835911694743592, 'reg_alpha': 9.145252563662213e-08, 'reg_lambda': 5.719576812624465e-08}. Best is trial 43 with value: 0.8966277681104197.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:33:52,819] Trial 46 finished with value: 0.8957795196079236 and parameters: {'n_estimators': 311, 'max_depth': 5, 'learning_rate': 0.024408073586212223, 'subsample': 0.8997150615845642, 'colsample_bytree': 0.8691869807480166, 'min_child_weight': 4, 'gamma': 0.702241612184297, 'reg_alpha': 3.371185026280849e-08, 'reg_lambda': 3.9100793856671855e-07}. Best is trial 43 with value: 0.8966277681104197.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:34:04,022] Trial 47 finished with value: 0.8920705235289941 and parameters: {'n_estimators': 172, 'max_depth': 8, 'learning_rate': 0.06304746147358538, 'subsample': 0.7921322298690603, 'colsample_bytree': 0.9396078614366178, 'min_child_weight': 1, 'gamma': 0.12457471322698677, 'reg_alpha': 2.568573597389443e-08, 'reg_lambda': 1.845193843281552e-05}. Best is trial 43 with value: 0.8966277681104197.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:34:19,514] Trial 48 finished with value: 0.892207813801226 and parameters: {'n_estimators': 268, 'max_depth': 6, 'learning_rate': 0.010434369419882641, 'subsample': 0.8775457315838003, 'colsample_bytree': 0.7663147842333401, 'min_child_weight': 3, 'gamma': 0.781869246635922, 'reg_alpha': 1.6145323971214134e-07, 'reg_lambda': 1.6071329754986605e-06}. Best is trial 43 with value: 0.8966277681104197.\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-10 16:34:37,983] Trial 49 finished with value: 0.8967409747108419 and parameters: {'n_estimators': 440, 'max_depth': 5, 'learning_rate': 0.01859925909414551, 'subsample': 0.8200477304861444, 'colsample_bytree': 0.999495136993001, 'min_child_weight': 1, 'gamma': 0.270167931432923, 'reg_alpha': 1.1510620732869116e-08, 'reg_lambda': 5.669623198561664e-07}. Best is trial 49 with value: 0.8967409747108419.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed in 862.51 seconds (14.38 minutes)\n",
      "Best parameters: {'n_estimators': 440, 'max_depth': 5, 'learning_rate': 0.01859925909414551, 'subsample': 0.8200477304861444, 'colsample_bytree': 0.999495136993001, 'min_child_weight': 1, 'gamma': 0.270167931432923, 'reg_alpha': 1.1510620732869116e-08, 'reg_lambda': 5.669623198561664e-07}\n",
      "Best CV score: 0.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaskazutin/PyCharmMiscProject/.venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[16:34:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9427\n",
      "CV score: 0.8967\n",
      "Potential overfitting: 0.0459\n",
      "Number of finished trials: 50\n",
      "Best xgboost parameters: {'n_estimators': 440, 'max_depth': 5, 'learning_rate': 0.01859925909414551, 'subsample': 0.8200477304861444, 'colsample_bytree': 0.999495136993001, 'min_child_weight': 1, 'gamma': 0.270167931432923, 'reg_alpha': 1.1510620732869116e-08, 'reg_lambda': 5.669623198561664e-07}\n",
      "Best CV score: 0.8967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:34:47,360] A new study created in memory with name: no-name-73c0eb90-b8d0-4c9b-9e84-b2d4bb40c96f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning catboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:35:04,464] Trial 0 finished with value: 0.891737613771651 and parameters: {'iterations': 234, 'depth': 9, 'learning_rate': 0.01824455348419317, 'l2_leaf_reg': 2.2068144276886437, 'bagging_temperature': 2.0504142682134283, 'random_strength': 1.19913036320027e-08, 'border_count': 130, 'grow_policy': 'Depthwise'}. Best is trial 0 with value: 0.891737613771651.\n",
      "[I 2025-03-10 16:35:23,946] Trial 1 finished with value: 0.8890108782331204 and parameters: {'iterations': 236, 'depth': 9, 'learning_rate': 0.018063255367123672, 'l2_leaf_reg': 0.12790573751215514, 'bagging_temperature': 9.178019357706752, 'random_strength': 3.6959090845264e-07, 'border_count': 182, 'grow_policy': 'Depthwise'}. Best is trial 0 with value: 0.891737613771651.\n",
      "[I 2025-03-10 16:35:38,006] Trial 2 finished with value: 0.8954444203411244 and parameters: {'iterations': 232, 'depth': 9, 'learning_rate': 0.021058401777615234, 'l2_leaf_reg': 0.17630764498301155, 'bagging_temperature': 7.154429498633785, 'random_strength': 1.2097456773669644e-08, 'border_count': 101, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:35:43,789] Trial 3 finished with value: 0.8910661787046912 and parameters: {'iterations': 258, 'depth': 8, 'learning_rate': 0.19774078299048495, 'l2_leaf_reg': 2.7981931465688636, 'bagging_temperature': 8.512729463736886, 'random_strength': 6.095195066479558e-09, 'border_count': 91, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:35:46,529] Trial 4 finished with value: 0.8941960410680331 and parameters: {'iterations': 125, 'depth': 7, 'learning_rate': 0.10764679944149597, 'l2_leaf_reg': 0.4331207136300161, 'bagging_temperature': 7.377360872203673, 'random_strength': 0.11556535010095595, 'border_count': 65, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:35:51,786] Trial 5 finished with value: 0.8869537053743773 and parameters: {'iterations': 176, 'depth': 8, 'learning_rate': 0.12712749284090483, 'l2_leaf_reg': 0.16467046360765702, 'bagging_temperature': 7.007562886785149, 'random_strength': 6.041736110732894e-08, 'border_count': 205, 'grow_policy': 'Depthwise'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:35:58,255] Trial 6 finished with value: 0.8821585185236096 and parameters: {'iterations': 210, 'depth': 8, 'learning_rate': 0.23296545821440132, 'l2_leaf_reg': 0.21889027643969963, 'bagging_temperature': 6.618636784579667, 'random_strength': 0.2290717095641775, 'border_count': 46, 'grow_policy': 'Lossguide'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:01,183] Trial 7 finished with value: 0.895077906380774 and parameters: {'iterations': 132, 'depth': 7, 'learning_rate': 0.09304151318199685, 'l2_leaf_reg': 0.1332175264769398, 'bagging_temperature': 5.421321199046641, 'random_strength': 0.7277806404409413, 'border_count': 181, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:06,487] Trial 8 finished with value: 0.8931097594771549 and parameters: {'iterations': 125, 'depth': 7, 'learning_rate': 0.020445506823583492, 'l2_leaf_reg': 1.3169980979534648, 'bagging_temperature': 6.113953971022642, 'random_strength': 2.0980312979808067e-09, 'border_count': 226, 'grow_policy': 'Lossguide'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:11,043] Trial 9 finished with value: 0.8932527461138979 and parameters: {'iterations': 282, 'depth': 5, 'learning_rate': 0.011769888483739475, 'l2_leaf_reg': 2.9023698907461273, 'bagging_temperature': 4.367352318589085, 'random_strength': 0.021001951799726918, 'border_count': 130, 'grow_policy': 'Depthwise'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:15,254] Trial 10 finished with value: 0.8934092488630994 and parameters: {'iterations': 55, 'depth': 10, 'learning_rate': 0.04334173506353441, 'l2_leaf_reg': 0.5488945256966067, 'bagging_temperature': 0.028846019481747476, 'random_strength': 2.6409855818462363e-05, 'border_count': 97, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:17,316] Trial 11 finished with value: 0.8680037080476879 and parameters: {'iterations': 133, 'depth': 5, 'learning_rate': 0.04860563968289028, 'l2_leaf_reg': 7.968209076965584, 'bagging_temperature': 4.200173361948707, 'random_strength': 9.288742732441458, 'border_count': 169, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:19,803] Trial 12 finished with value: 0.8943573121820512 and parameters: {'iterations': 177, 'depth': 6, 'learning_rate': 0.08560781452571312, 'l2_leaf_reg': 0.31863039471261456, 'bagging_temperature': 5.134994037295219, 'random_strength': 0.0001144826536097458, 'border_count': 157, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:25,852] Trial 13 finished with value: 0.8942185694561475 and parameters: {'iterations': 74, 'depth': 10, 'learning_rate': 0.03112084120621158, 'l2_leaf_reg': 0.10245538518007555, 'bagging_temperature': 2.7200881867967115, 'random_strength': 6.97140488077154e-06, 'border_count': 239, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:28,562] Trial 14 finished with value: 0.8934909839455066 and parameters: {'iterations': 204, 'depth': 6, 'learning_rate': 0.07911729848478895, 'l2_leaf_reg': 0.7046371061545055, 'bagging_temperature': 8.177742103331667, 'random_strength': 0.0008514616100042151, 'border_count': 100, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:30,369] Trial 15 finished with value: 0.8909141235726302 and parameters: {'iterations': 149, 'depth': 4, 'learning_rate': 0.03250013064173676, 'l2_leaf_reg': 0.2780208483529413, 'bagging_temperature': 9.771512268212685, 'random_strength': 1.0757215745916072e-06, 'border_count': 195, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:40,484] Trial 16 finished with value: 0.8673490741658393 and parameters: {'iterations': 298, 'depth': 9, 'learning_rate': 0.011471327055256126, 'l2_leaf_reg': 0.18017826406174572, 'bagging_temperature': 5.530506189305901, 'random_strength': 8.404422281312973, 'border_count': 140, 'grow_policy': 'Lossguide'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:42,464] Trial 17 finished with value: 0.8950143905980715 and parameters: {'iterations': 105, 'depth': 6, 'learning_rate': 0.06546105674288633, 'l2_leaf_reg': 0.9922254482200392, 'bagging_temperature': 2.8338343227585447, 'random_strength': 0.002577517893766014, 'border_count': 32, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:47,017] Trial 18 finished with value: 0.888120997242218 and parameters: {'iterations': 205, 'depth': 8, 'learning_rate': 0.1655398340187769, 'l2_leaf_reg': 0.10225686317467317, 'bagging_temperature': 3.6333987252937816, 'random_strength': 0.0006100967096728103, 'border_count': 113, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:36:50,744] Trial 19 finished with value: 0.8941683769040628 and parameters: {'iterations': 92, 'depth': 7, 'learning_rate': 0.03590309883083154, 'l2_leaf_reg': 0.3749494077160361, 'bagging_temperature': 7.930543210454112, 'random_strength': 0.46479432513334756, 'border_count': 71, 'grow_policy': 'Lossguide'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:37:01,018] Trial 20 finished with value: 0.8912936717690971 and parameters: {'iterations': 154, 'depth': 10, 'learning_rate': 0.2811385168197198, 'l2_leaf_reg': 8.28280647013018, 'bagging_temperature': 5.825020725359809, 'random_strength': 0.006974405551883761, 'border_count': 211, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:37:03,039] Trial 21 finished with value: 0.8947153772336435 and parameters: {'iterations': 108, 'depth': 6, 'learning_rate': 0.05559134428563208, 'l2_leaf_reg': 1.3562695854229552, 'bagging_temperature': 1.016384347078474, 'random_strength': 0.002731728015414736, 'border_count': 39, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:37:04,706] Trial 22 finished with value: 0.8912966918619502 and parameters: {'iterations': 99, 'depth': 5, 'learning_rate': 0.05928952337347925, 'l2_leaf_reg': 0.8366617153561247, 'bagging_temperature': 3.3295624444041563, 'random_strength': 1.2347651612866768, 'border_count': 75, 'grow_policy': 'SymmetricTree'}. Best is trial 2 with value: 0.8954444203411244.\n",
      "[I 2025-03-10 16:37:07,447] Trial 23 finished with value: 0.8966482534712762 and parameters: {'iterations': 153, 'depth': 7, 'learning_rate': 0.0744675890512797, 'l2_leaf_reg': 4.211113530816746, 'bagging_temperature': 1.7139892802196188, 'random_strength': 0.07336444861048914, 'border_count': 158, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:10,282] Trial 24 finished with value: 0.8964812125099406 and parameters: {'iterations': 158, 'depth': 7, 'learning_rate': 0.12822315280890487, 'l2_leaf_reg': 5.7384405744777975, 'bagging_temperature': 4.530801864607806, 'random_strength': 0.034308216115177266, 'border_count': 161, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:13,180] Trial 25 finished with value: 0.8962468560225328 and parameters: {'iterations': 159, 'depth': 7, 'learning_rate': 0.14836500849845669, 'l2_leaf_reg': 5.199674612642746, 'bagging_temperature': 1.6892964048896708, 'random_strength': 0.03672053983954721, 'border_count': 152, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:16,087] Trial 26 finished with value: 0.8954462467753099 and parameters: {'iterations': 157, 'depth': 7, 'learning_rate': 0.14366505827953938, 'l2_leaf_reg': 4.9226583365335586, 'bagging_temperature': 1.3657630244923966, 'random_strength': 0.04136954807265401, 'border_count': 156, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:18,692] Trial 27 finished with value: 0.8952743802831835 and parameters: {'iterations': 187, 'depth': 6, 'learning_rate': 0.11906543298357465, 'l2_leaf_reg': 5.224755037795132, 'bagging_temperature': 1.0405719008845016, 'random_strength': 0.025212917240335, 'border_count': 149, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:23,726] Trial 28 finished with value: 0.8931374854035944 and parameters: {'iterations': 154, 'depth': 8, 'learning_rate': 0.1645791905278574, 'l2_leaf_reg': 4.832634182976578, 'bagging_temperature': 2.0512206146267933, 'random_strength': 0.0003146589611513222, 'border_count': 122, 'grow_policy': 'Lossguide'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:27,796] Trial 29 finished with value: 0.8878693173755012 and parameters: {'iterations': 191, 'depth': 7, 'learning_rate': 0.2985157532724683, 'l2_leaf_reg': 1.7850318697469687, 'bagging_temperature': 0.010528030328899263, 'random_strength': 1.6588508327327696, 'border_count': 174, 'grow_policy': 'Depthwise'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:30,139] Trial 30 finished with value: 0.8922138731089341 and parameters: {'iterations': 167, 'depth': 4, 'learning_rate': 0.07067091479485713, 'l2_leaf_reg': 3.4333227624005502, 'bagging_temperature': 2.067343981462697, 'random_strength': 0.123110895064619, 'border_count': 132, 'grow_policy': 'Depthwise'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:32,822] Trial 31 finished with value: 0.895644141653903 and parameters: {'iterations': 146, 'depth': 7, 'learning_rate': 0.15155711400230087, 'l2_leaf_reg': 5.039642271784219, 'bagging_temperature': 1.0951749944339442, 'random_strength': 0.033704005580785894, 'border_count': 158, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:35,491] Trial 32 finished with value: 0.8960036737787067 and parameters: {'iterations': 142, 'depth': 7, 'learning_rate': 0.10157426950519403, 'l2_leaf_reg': 6.650753787568529, 'bagging_temperature': 1.4322462902138984, 'random_strength': 0.014088145375673303, 'border_count': 165, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:38,610] Trial 33 finished with value: 0.896236260609804 and parameters: {'iterations': 118, 'depth': 8, 'learning_rate': 0.10048826445315025, 'l2_leaf_reg': 9.827611752984938, 'bagging_temperature': 1.686440645030322, 'random_strength': 0.009351551104346711, 'border_count': 189, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:43,283] Trial 34 finished with value: 0.8947769078688562 and parameters: {'iterations': 115, 'depth': 9, 'learning_rate': 0.1804735741477398, 'l2_leaf_reg': 9.540116273548826, 'bagging_temperature': 2.7479702859682567, 'random_strength': 0.0035624506233916772, 'border_count': 185, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:46,161] Trial 35 finished with value: 0.8953502650146348 and parameters: {'iterations': 78, 'depth': 8, 'learning_rate': 0.1281619099578964, 'l2_leaf_reg': 3.866087171817866, 'bagging_temperature': 4.423324387070052, 'random_strength': 0.0966511762885901, 'border_count': 254, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:54,051] Trial 36 finished with value: 0.8888985218745121 and parameters: {'iterations': 228, 'depth': 9, 'learning_rate': 0.20653731150961005, 'l2_leaf_reg': 2.1328008123131177, 'bagging_temperature': 0.43334153898504835, 'random_strength': 0.0011084487582930357, 'border_count': 189, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:37:59,947] Trial 37 finished with value: 0.8946770060793033 and parameters: {'iterations': 168, 'depth': 8, 'learning_rate': 0.07648005146856185, 'l2_leaf_reg': 7.154087611353365, 'bagging_temperature': 1.9189193372078726, 'random_strength': 1.9545530434627305, 'border_count': 204, 'grow_policy': 'Depthwise'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:38:03,508] Trial 38 finished with value: 0.8965092079554011 and parameters: {'iterations': 120, 'depth': 8, 'learning_rate': 0.10324367617990783, 'l2_leaf_reg': 9.949622473189006, 'bagging_temperature': 3.5139748823573873, 'random_strength': 0.2746187830748307, 'border_count': 141, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:38:06,919] Trial 39 finished with value: 0.8944010284028158 and parameters: {'iterations': 135, 'depth': 8, 'learning_rate': 0.22674629928721185, 'l2_leaf_reg': 5.857812713694474, 'bagging_temperature': 3.5764807371167597, 'random_strength': 0.2817714895063074, 'border_count': 144, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:38:11,751] Trial 40 finished with value: 0.8923408043947421 and parameters: {'iterations': 187, 'depth': 6, 'learning_rate': 0.11650483251191299, 'l2_leaf_reg': 4.09239157826226, 'bagging_temperature': 4.6458264679502355, 'random_strength': 0.11618438308024387, 'border_count': 113, 'grow_policy': 'Lossguide'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:38:14,276] Trial 41 finished with value: 0.89607743209801 and parameters: {'iterations': 121, 'depth': 7, 'learning_rate': 0.09213929467828724, 'l2_leaf_reg': 9.740211953495457, 'bagging_temperature': 3.187389406714968, 'random_strength': 0.008250903703363536, 'border_count': 175, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:38:16,909] Trial 42 finished with value: 0.89527042874989 and parameters: {'iterations': 91, 'depth': 8, 'learning_rate': 0.1371522486199266, 'l2_leaf_reg': 2.873464105280348, 'bagging_temperature': 2.3217267271792137, 'random_strength': 0.054494196629215896, 'border_count': 140, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:38:19,805] Trial 43 finished with value: 0.8966236614639971 and parameters: {'iterations': 164, 'depth': 7, 'learning_rate': 0.1039758828219095, 'l2_leaf_reg': 6.323493489466512, 'bagging_temperature': 1.6266043793791265, 'random_strength': 0.383559392949076, 'border_count': 165, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:38:22,610] Trial 44 finished with value: 0.8945127197249534 and parameters: {'iterations': 166, 'depth': 7, 'learning_rate': 0.10804698617579885, 'l2_leaf_reg': 6.31814861638557, 'bagging_temperature': 3.943831071619492, 'random_strength': 3.339538735149392, 'border_count': 121, 'grow_policy': 'SymmetricTree'}. Best is trial 23 with value: 0.8966482534712762.\n",
      "[I 2025-03-10 16:38:25,590] Trial 45 finished with value: 0.8973929953649126 and parameters: {'iterations': 177, 'depth': 7, 'learning_rate': 0.08480695964963093, 'l2_leaf_reg': 3.6940324223568908, 'bagging_temperature': 0.5432044147152886, 'random_strength': 0.4653821588806542, 'border_count': 164, 'grow_policy': 'SymmetricTree'}. Best is trial 45 with value: 0.8973929953649126.\n",
      "[I 2025-03-10 16:38:29,195] Trial 46 finished with value: 0.8981811340468143 and parameters: {'iterations': 219, 'depth': 7, 'learning_rate': 0.04432388352713773, 'l2_leaf_reg': 2.0347149523243866, 'bagging_temperature': 0.5233595737286851, 'random_strength': 0.45418597524276966, 'border_count': 165, 'grow_policy': 'SymmetricTree'}. Best is trial 46 with value: 0.8981811340468143.\n",
      "[I 2025-03-10 16:38:33,779] Trial 47 finished with value: 0.8958757823044866 and parameters: {'iterations': 262, 'depth': 6, 'learning_rate': 0.03730780239273626, 'l2_leaf_reg': 2.4420907756961205, 'bagging_temperature': 0.43537849128706035, 'random_strength': 0.635502257802788, 'border_count': 171, 'grow_policy': 'Depthwise'}. Best is trial 46 with value: 0.8981811340468143.\n",
      "[I 2025-03-10 16:38:39,003] Trial 48 finished with value: 0.892597877108099 and parameters: {'iterations': 221, 'depth': 8, 'learning_rate': 0.0519560602178875, 'l2_leaf_reg': 1.6190768446951433, 'bagging_temperature': 0.5232208853502258, 'random_strength': 4.80678238111304, 'border_count': 217, 'grow_policy': 'SymmetricTree'}. Best is trial 46 with value: 0.8981811340468143.\n",
      "[I 2025-03-10 16:38:42,117] Trial 49 finished with value: 0.8961782390201835 and parameters: {'iterations': 256, 'depth': 6, 'learning_rate': 0.045910161954241545, 'l2_leaf_reg': 3.4838099565621925, 'bagging_temperature': 0.7897102130357616, 'random_strength': 0.2728746623486201, 'border_count': 200, 'grow_policy': 'SymmetricTree'}. Best is trial 46 with value: 0.8981811340468143.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed in 234.76 seconds (3.91 minutes)\n",
      "Best parameters: {'iterations': 219, 'depth': 7, 'learning_rate': 0.04432388352713773, 'l2_leaf_reg': 2.0347149523243866, 'bagging_temperature': 0.5233595737286851, 'random_strength': 0.45418597524276966, 'border_count': 165, 'grow_policy': 'SymmetricTree'}\n",
      "Best CV score: 0.8982\n",
      "Train score: 0.9333\n",
      "CV score: 0.8982\n",
      "Potential overfitting: 0.0352\n",
      "Number of finished trials: 50\n",
      "Best catboost parameters: {'iterations': 219, 'depth': 7, 'learning_rate': 0.04432388352713773, 'l2_leaf_reg': 2.0347149523243866, 'bagging_temperature': 0.5233595737286851, 'random_strength': 0.45418597524276966, 'border_count': 165, 'grow_policy': 'SymmetricTree'}\n",
      "Best CV score: 0.8982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total hyperparameter tuning time: 1170.66 seconds (19.51 minutes)\n",
      "Generating meta-features from logistic...\n",
      "Generating meta-features from xgboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:38:47,240] A new study created in memory with name: no-name-a1b7b7d2-b9e5-4fad-9c77-317113c82fa8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating meta-features from catboost...\n",
      "Generated meta-features from 3 models: logistic, xgboost, catboost\n",
      "Optimizing meta-learner: logistic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:38:47,434] Trial 0 finished with value: 0.9396623387503207 and parameters: {'C': 0.05735268878335289}. Best is trial 0 with value: 0.9396623387503207.\n",
      "[I 2025-03-10 16:38:47,878] Trial 1 finished with value: 0.945991691056965 and parameters: {'C': 0.35143486017780406}. Best is trial 1 with value: 0.945991691056965.\n",
      "[I 2025-03-10 16:38:48,120] Trial 2 finished with value: 0.9469822377019629 and parameters: {'C': 0.9814222034646726}. Best is trial 2 with value: 0.9469822377019629.\n",
      "[I 2025-03-10 16:38:48,377] Trial 3 finished with value: 0.9459772203044583 and parameters: {'C': 0.34812073150419176}. Best is trial 2 with value: 0.9469822377019629.\n",
      "[I 2025-03-10 16:38:48,555] Trial 4 finished with value: 0.935873418780876 and parameters: {'C': 0.021719188239556556}. Best is trial 2 with value: 0.9469822377019629.\n",
      "[I 2025-03-10 16:38:48,894] Trial 5 finished with value: 0.9419842342111824 and parameters: {'C': 0.09710330139210134}. Best is trial 2 with value: 0.9469822377019629.\n",
      "[I 2025-03-10 16:38:49,025] Trial 6 finished with value: 0.9323304937531578 and parameters: {'C': 0.0028931625227286515}. Best is trial 2 with value: 0.9469822377019629.\n",
      "[I 2025-03-10 16:38:49,265] Trial 7 finished with value: 0.9429499578324755 and parameters: {'C': 0.12290229855493848}. Best is trial 2 with value: 0.9469822377019629.\n",
      "[I 2025-03-10 16:38:49,677] Trial 8 finished with value: 0.947102122019533 and parameters: {'C': 1.408121832887673}. Best is trial 8 with value: 0.947102122019533.\n",
      "[I 2025-03-10 16:38:49,838] Trial 9 finished with value: 0.9318726612644038 and parameters: {'C': 0.0014352514019243692}. Best is trial 8 with value: 0.947102122019533.\n",
      "[I 2025-03-10 16:38:50,041] Trial 10 finished with value: 0.9471479472653487 and parameters: {'C': 5.372144810883008}. Best is trial 10 with value: 0.9471479472653487.\n",
      "[I 2025-03-10 16:38:50,331] Trial 11 finished with value: 0.9471541557313436 and parameters: {'C': 9.170038046569024}. Best is trial 11 with value: 0.9471541557313436.\n",
      "[I 2025-03-10 16:38:50,571] Trial 12 finished with value: 0.9471537381341314 and parameters: {'C': 7.6854070794027205}. Best is trial 11 with value: 0.9471541557313436.\n",
      "[I 2025-03-10 16:38:50,857] Trial 13 finished with value: 0.9471458841825987 and parameters: {'C': 6.089700555797207}. Best is trial 11 with value: 0.9471541557313436.\n",
      "[I 2025-03-10 16:38:51,228] Trial 14 finished with value: 0.9471533329415583 and parameters: {'C': 9.518615179527501}. Best is trial 11 with value: 0.9471541557313436.\n",
      "[I 2025-03-10 16:38:51,495] Trial 15 finished with value: 0.947168668364361 and parameters: {'C': 2.175062852129673}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:51,780] Trial 16 finished with value: 0.947148415945466 and parameters: {'C': 1.9506217111390936}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:52,044] Trial 17 finished with value: 0.9471492447191583 and parameters: {'C': 1.962909768592054}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:52,431] Trial 18 finished with value: 0.9466982426141181 and parameters: {'C': 0.6246509240075745}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:52,679] Trial 19 finished with value: 0.9471446597880202 and parameters: {'C': 3.326819512660063}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:52,852] Trial 20 finished with value: 0.9354515425997357 and parameters: {'C': 0.018812678862232743}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:53,055] Trial 21 finished with value: 0.9471537464295363 and parameters: {'C': 9.447216944795706}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:53,259] Trial 22 finished with value: 0.9471516841187103 and parameters: {'C': 3.1384463581318034}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:53,511] Trial 23 finished with value: 0.9471434103610488 and parameters: {'C': 3.561049845722037}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:53,720] Trial 24 finished with value: 0.9466209426446227 and parameters: {'C': 0.5680765409044752}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:53,918] Trial 25 finished with value: 0.9471512643288234 and parameters: {'C': 9.649363207380269}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:54,115] Trial 26 finished with value: 0.9471545721480166 and parameters: {'C': 3.8673959432024194}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:54,342] Trial 27 finished with value: 0.9469867860440738 and parameters: {'C': 0.9842532272576445}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:54,516] Trial 28 finished with value: 0.9471504436376799 and parameters: {'C': 3.128208834791447}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:54,707] Trial 29 finished with value: 0.9435393009701499 and parameters: {'C': 0.14388647889645148}. Best is trial 15 with value: 0.947168668364361.\n",
      "[I 2025-03-10 16:38:54,751] A new study created in memory with name: no-name-cf77111f-f082-4223-bb17-494d257e8af1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic meta-learner params: {'C': 2.175062852129673}\n",
      "Optimizing meta-learner: xgb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:38:56,006] Trial 0 finished with value: 0.9494575348855477 and parameters: {'n_estimators': 168, 'max_depth': 5, 'learning_rate': 0.027622414351761226}. Best is trial 0 with value: 0.9494575348855477.\n",
      "[I 2025-03-10 16:38:56,470] Trial 1 finished with value: 0.949917224913691 and parameters: {'n_estimators': 88, 'max_depth': 4, 'learning_rate': 0.09332078477810833}. Best is trial 1 with value: 0.949917224913691.\n",
      "[I 2025-03-10 16:38:56,798] Trial 2 finished with value: 0.945414252591223 and parameters: {'n_estimators': 75, 'max_depth': 3, 'learning_rate': 0.016381447910032713}. Best is trial 1 with value: 0.949917224913691.\n",
      "[I 2025-03-10 16:38:57,516] Trial 3 finished with value: 0.9482362180753375 and parameters: {'n_estimators': 140, 'max_depth': 4, 'learning_rate': 0.14528732955915255}. Best is trial 1 with value: 0.949917224913691.\n",
      "[I 2025-03-10 16:38:58,427] Trial 4 finished with value: 0.9496163409641014 and parameters: {'n_estimators': 174, 'max_depth': 4, 'learning_rate': 0.029201633689695465}. Best is trial 1 with value: 0.949917224913691.\n",
      "[I 2025-03-10 16:38:58,738] Trial 5 finished with value: 0.9499408624647131 and parameters: {'n_estimators': 108, 'max_depth': 2, 'learning_rate': 0.18224529769604428}. Best is trial 5 with value: 0.9499408624647131.\n",
      "[I 2025-03-10 16:38:59,177] Trial 6 finished with value: 0.9462115828202251 and parameters: {'n_estimators': 64, 'max_depth': 5, 'learning_rate': 0.011066898341853234}. Best is trial 5 with value: 0.9499408624647131.\n",
      "[I 2025-03-10 16:38:59,871] Trial 7 finished with value: 0.9482088560879884 and parameters: {'n_estimators': 110, 'max_depth': 5, 'learning_rate': 0.13988948472602114}. Best is trial 5 with value: 0.9499408624647131.\n",
      "[I 2025-03-10 16:39:00,540] Trial 8 finished with value: 0.9486598713046238 and parameters: {'n_estimators': 59, 'max_depth': 5, 'learning_rate': 0.034873031805328504}. Best is trial 5 with value: 0.9499408624647131.\n",
      "[I 2025-03-10 16:39:01,181] Trial 9 finished with value: 0.9500017385194779 and parameters: {'n_estimators': 178, 'max_depth': 2, 'learning_rate': 0.08841294688011668}. Best is trial 9 with value: 0.9500017385194779.\n",
      "[I 2025-03-10 16:39:01,938] Trial 10 finished with value: 0.9499471832110306 and parameters: {'n_estimators': 199, 'max_depth': 2, 'learning_rate': 0.07505474811018491}. Best is trial 9 with value: 0.9500017385194779.\n",
      "[I 2025-03-10 16:39:02,737] Trial 11 finished with value: 0.9498110828934276 and parameters: {'n_estimators': 199, 'max_depth': 2, 'learning_rate': 0.0626636503904555}. Best is trial 9 with value: 0.9500017385194779.\n",
      "[I 2025-03-10 16:39:03,376] Trial 12 finished with value: 0.9498142936403982 and parameters: {'n_estimators': 190, 'max_depth': 2, 'learning_rate': 0.06413446604243672}. Best is trial 9 with value: 0.9500017385194779.\n",
      "[I 2025-03-10 16:39:04,163] Trial 13 finished with value: 0.9497626780130606 and parameters: {'n_estimators': 152, 'max_depth': 3, 'learning_rate': 0.08694100936962076}. Best is trial 9 with value: 0.9500017385194779.\n",
      "[I 2025-03-10 16:39:05,071] Trial 14 finished with value: 0.9499468171331209 and parameters: {'n_estimators': 176, 'max_depth': 3, 'learning_rate': 0.0503343371240618}. Best is trial 9 with value: 0.9500017385194779.\n",
      "[I 2025-03-10 16:39:05,626] Trial 15 finished with value: 0.9499315901002463 and parameters: {'n_estimators': 149, 'max_depth': 2, 'learning_rate': 0.1021340889474718}. Best is trial 9 with value: 0.9500017385194779.\n",
      "[I 2025-03-10 16:39:06,819] Trial 16 finished with value: 0.950079599001975 and parameters: {'n_estimators': 198, 'max_depth': 3, 'learning_rate': 0.04894343215741336}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:07,518] Trial 17 finished with value: 0.9493390156296788 and parameters: {'n_estimators': 132, 'max_depth': 3, 'learning_rate': 0.04137916959040461}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:08,424] Trial 18 finished with value: 0.9479471110763132 and parameters: {'n_estimators': 165, 'max_depth': 3, 'learning_rate': 0.019815553695655193}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:09,015] Trial 19 finished with value: 0.9493262224172178 and parameters: {'n_estimators': 184, 'max_depth': 2, 'learning_rate': 0.049750102479465194}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:09,908] Trial 20 finished with value: 0.9493974863260564 and parameters: {'n_estimators': 156, 'max_depth': 3, 'learning_rate': 0.12295872111941063}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:10,522] Trial 21 finished with value: 0.9499685093240793 and parameters: {'n_estimators': 197, 'max_depth': 2, 'learning_rate': 0.0715698859706274}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:11,091] Trial 22 finished with value: 0.9499542016809113 and parameters: {'n_estimators': 186, 'max_depth': 2, 'learning_rate': 0.07503074092189144}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:11,721] Trial 23 finished with value: 0.9495866399093698 and parameters: {'n_estimators': 189, 'max_depth': 2, 'learning_rate': 0.05467066124082706}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:12,473] Trial 24 finished with value: 0.9494951609344662 and parameters: {'n_estimators': 163, 'max_depth': 3, 'learning_rate': 0.03914011196482996}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:13,057] Trial 25 finished with value: 0.9500079619106192 and parameters: {'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.10656130300287772}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:13,949] Trial 26 finished with value: 0.946358350059529 and parameters: {'n_estimators': 179, 'max_depth': 4, 'learning_rate': 0.18523459327809474}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:14,423] Trial 27 finished with value: 0.9498769225999532 and parameters: {'n_estimators': 123, 'max_depth': 3, 'learning_rate': 0.10646269329240936}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:14,923] Trial 28 finished with value: 0.9500749434615884 and parameters: {'n_estimators': 174, 'max_depth': 2, 'learning_rate': 0.12415247187381331}. Best is trial 16 with value: 0.950079599001975.\n",
      "[I 2025-03-10 16:39:15,405] Trial 29 finished with value: 0.9501464810627956 and parameters: {'n_estimators': 166, 'max_depth': 2, 'learning_rate': 0.1553466598669551}. Best is trial 29 with value: 0.9501464810627956.\n",
      "[I 2025-03-10 16:39:15,492] A new study created in memory with name: no-name-351df2c3-7ea7-45c3-b260-b20fddc42e8f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost meta-learner params: {'n_estimators': 166, 'max_depth': 2, 'learning_rate': 0.1553466598669551}\n",
      "Optimizing meta-learner: catboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:39:15,795] Trial 0 finished with value: 0.9476585205265089 and parameters: {'iterations': 124, 'depth': 3, 'learning_rate': 0.03951486960092621}. Best is trial 0 with value: 0.9476585205265089.\n",
      "[I 2025-03-10 16:39:16,280] Trial 1 finished with value: 0.9506025474595102 and parameters: {'iterations': 131, 'depth': 5, 'learning_rate': 0.09013621181882027}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:16,407] Trial 2 finished with value: 0.940126136870855 and parameters: {'iterations': 57, 'depth': 2, 'learning_rate': 0.011380674316722064}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:17,041] Trial 3 finished with value: 0.9503796795171541 and parameters: {'iterations': 142, 'depth': 5, 'learning_rate': 0.14871630164564323}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:17,903] Trial 4 finished with value: 0.9496407840733937 and parameters: {'iterations': 185, 'depth': 5, 'learning_rate': 0.15099550441086265}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:18,182] Trial 5 finished with value: 0.9479815016121522 and parameters: {'iterations': 100, 'depth': 4, 'learning_rate': 0.03452315624521006}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:18,624] Trial 6 finished with value: 0.9502117402457069 and parameters: {'iterations': 114, 'depth': 5, 'learning_rate': 0.17423028895150297}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:18,923] Trial 7 finished with value: 0.9495689017286152 and parameters: {'iterations': 109, 'depth': 3, 'learning_rate': 0.0824021808597083}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:19,259] Trial 8 finished with value: 0.9501813485912534 and parameters: {'iterations': 142, 'depth': 3, 'learning_rate': 0.10761398635095323}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:19,486] Trial 9 finished with value: 0.9460772913962661 and parameters: {'iterations': 67, 'depth': 4, 'learning_rate': 0.029457160910355138}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:20,017] Trial 10 finished with value: 0.9504970694080175 and parameters: {'iterations': 186, 'depth': 4, 'learning_rate': 0.06937058993484226}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:20,582] Trial 11 finished with value: 0.950528341541822 and parameters: {'iterations': 199, 'depth': 4, 'learning_rate': 0.06611193892612295}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:21,133] Trial 12 finished with value: 0.9504712526680692 and parameters: {'iterations': 161, 'depth': 5, 'learning_rate': 0.06021189154827612}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:21,657] Trial 13 finished with value: 0.948269485477983 and parameters: {'iterations': 194, 'depth': 4, 'learning_rate': 0.020110585463695204}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:21,980] Trial 14 finished with value: 0.9503662306304722 and parameters: {'iterations': 86, 'depth': 5, 'learning_rate': 0.09855245878556489}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:22,449] Trial 15 finished with value: 0.9501853398440122 and parameters: {'iterations': 161, 'depth': 4, 'learning_rate': 0.06063145447167078}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:22,796] Trial 16 finished with value: 0.9486564488974573 and parameters: {'iterations': 168, 'depth': 2, 'learning_rate': 0.0495953200959364}. Best is trial 1 with value: 0.9506025474595102.\n",
      "[I 2025-03-10 16:39:23,298] Trial 17 finished with value: 0.9506761354015921 and parameters: {'iterations': 140, 'depth': 5, 'learning_rate': 0.10740440074050665}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:23,762] Trial 18 finished with value: 0.9504622236532878 and parameters: {'iterations': 133, 'depth': 5, 'learning_rate': 0.11114290497832728}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:24,279] Trial 19 finished with value: 0.9494973307243841 and parameters: {'iterations': 151, 'depth': 5, 'learning_rate': 0.1932938774481257}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:24,571] Trial 20 finished with value: 0.9503631083098332 and parameters: {'iterations': 83, 'depth': 5, 'learning_rate': 0.12529838332191404}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:25,060] Trial 21 finished with value: 0.9504172404449218 and parameters: {'iterations': 174, 'depth': 4, 'learning_rate': 0.08605499591144486}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:25,423] Trial 22 finished with value: 0.9492722624122079 and parameters: {'iterations': 125, 'depth': 4, 'learning_rate': 0.05047021163394527}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:26,092] Trial 23 finished with value: 0.9504501930218214 and parameters: {'iterations': 199, 'depth': 5, 'learning_rate': 0.08095003534670808}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:26,464] Trial 24 finished with value: 0.9471569142919819 and parameters: {'iterations': 146, 'depth': 3, 'learning_rate': 0.02430049959009113}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:26,741] Trial 25 finished with value: 0.9504027025469941 and parameters: {'iterations': 94, 'depth': 4, 'learning_rate': 0.13100693111258654}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:27,142] Trial 26 finished with value: 0.950360772700722 and parameters: {'iterations': 118, 'depth': 5, 'learning_rate': 0.06924755439016586}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:27,804] Trial 27 finished with value: 0.95060702307236 and parameters: {'iterations': 136, 'depth': 5, 'learning_rate': 0.09325786809647305}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:28,413] Trial 28 finished with value: 0.9506159748819176 and parameters: {'iterations': 134, 'depth': 5, 'learning_rate': 0.09672781386148777}. Best is trial 17 with value: 0.9506761354015921.\n",
      "[I 2025-03-10 16:39:29,028] Trial 29 finished with value: 0.9495129936395813 and parameters: {'iterations': 134, 'depth': 5, 'learning_rate': 0.0408385274663836}. Best is trial 17 with value: 0.9506761354015921.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost meta-learner params: {'iterations': 140, 'depth': 5, 'learning_rate': 0.10740440074050665}\n",
      "Evaluating meta-learner: logistic...\n",
      "logistic - Accuracy: 0.8085, AUC: 0.8977\n",
      "Evaluating meta-learner: xgb...\n",
      "xgb - Accuracy: 0.8016, AUC: 0.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:39:29,326] A new study created in memory with name: no-name-3b8eea49-5a6b-4ccb-a8a4-5df622d5e9dc\n",
      "[I 2025-03-10 16:39:29,330] Trial 0 finished with value: 0.9010050424080804 and parameters: {'weight_logistic': 0.4832886600597329, 'weight_xgb': 0.14959072687935046, 'weight_catboost': 0.3031909271692752}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,333] Trial 1 finished with value: 0.900844986957465 and parameters: {'weight_logistic': 0.8125293690812658, 'weight_xgb': 0.40670612634783376, 'weight_catboost': 0.4675163174821635}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,336] Trial 2 finished with value: 0.9004216998153409 and parameters: {'weight_logistic': 0.38890790775775463, 'weight_xgb': 0.824286063820643, 'weight_catboost': 0.5846646679177656}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,339] Trial 3 finished with value: 0.9008383730958693 and parameters: {'weight_logistic': 0.8030724236039787, 'weight_xgb': 0.57119103035189, 'weight_catboost': 0.5835138390949509}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,342] Trial 4 finished with value: 0.9009362582474854 and parameters: {'weight_logistic': 0.6262073878070781, 'weight_xgb': 0.43779230209929443, 'weight_catboost': 0.5565012693569009}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,345] Trial 5 finished with value: 0.9009230305242941 and parameters: {'weight_logistic': 0.7973537818670123, 'weight_xgb': 0.431637487165627, 'weight_catboost': 0.5181314258836028}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,348] Trial 6 finished with value: 0.9003330740699589 and parameters: {'weight_logistic': 0.26613852035149227, 'weight_xgb': 0.602570935863347, 'weight_catboost': 0.3849001895809303}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,351] Trial 7 finished with value: 0.9008516008190607 and parameters: {'weight_logistic': 0.4019101395707467, 'weight_xgb': 0.5845131355253308, 'weight_catboost': 0.9115237201007698}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,354] Trial 8 finished with value: 0.9005724958597225 and parameters: {'weight_logistic': 0.34667941773458816, 'weight_xgb': 0.8682792556965762, 'weight_catboost': 0.9368946381045531}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,357] Trial 9 finished with value: 0.9002880998111082 and parameters: {'weight_logistic': 0.7234095813131188, 'weight_xgb': 0.9144205842547197, 'weight_catboost': 0.19141588651054275}. Best is trial 0 with value: 0.9010050424080804.\n",
      "[I 2025-03-10 16:39:29,370] Trial 10 finished with value: 0.9011016047873777 and parameters: {'weight_logistic': 0.16685514672784763, 'weight_xgb': 0.10259651986602586, 'weight_catboost': 0.2160619360186556}. Best is trial 10 with value: 0.9011016047873777.\n",
      "[I 2025-03-10 16:39:29,383] Trial 11 finished with value: 0.9008251453726779 and parameters: {'weight_logistic': 0.13605873265368543, 'weight_xgb': 0.11624208790662116, 'weight_catboost': 0.12352256570620532}. Best is trial 10 with value: 0.9011016047873777.\n",
      "[I 2025-03-10 16:39:29,397] Trial 12 finished with value: 0.9011148325105689 and parameters: {'weight_logistic': 0.10056415271805712, 'weight_xgb': 0.10784596425109017, 'weight_catboost': 0.29280301303491457}. Best is trial 12 with value: 0.9011148325105689.\n",
      "[I 2025-03-10 16:39:29,404] Trial 13 finished with value: 0.9006015968507436 and parameters: {'weight_logistic': 0.1045791878713051, 'weight_xgb': 0.24605918327279278, 'weight_catboost': 0.27683058471264227}. Best is trial 12 with value: 0.9011148325105689.\n",
      "[I 2025-03-10 16:39:29,424] Trial 14 finished with value: 0.901038111716059 and parameters: {'weight_logistic': 0.965254709123595, 'weight_xgb': 0.26715611347958457, 'weight_catboost': 0.7413236831757177}. Best is trial 12 with value: 0.9011148325105689.\n",
      "[I 2025-03-10 16:39:29,432] Trial 15 finished with value: 0.9007695889352741 and parameters: {'weight_logistic': 0.22997514214831705, 'weight_xgb': 0.2587246052997712, 'weight_catboost': 0.24398158358084465}. Best is trial 12 with value: 0.9011148325105689.\n",
      "[I 2025-03-10 16:39:29,443] Trial 16 finished with value: 0.899661105731837 and parameters: {'weight_logistic': 0.17231275437518595, 'weight_xgb': 0.7105941563862627, 'weight_catboost': 0.11495718828719363}. Best is trial 12 with value: 0.9011148325105689.\n",
      "[I 2025-03-10 16:39:29,450] Trial 17 finished with value: 0.9012206542961001 and parameters: {'weight_logistic': 0.3109591013800358, 'weight_xgb': 0.10515491963385287, 'weight_catboost': 0.3835776687694389}. Best is trial 17 with value: 0.9012206542961001.\n",
      "[I 2025-03-10 16:39:29,458] Trial 18 finished with value: 0.9008859928993582 and parameters: {'weight_logistic': 0.31090047046139624, 'weight_xgb': 0.3044346386701928, 'weight_catboost': 0.3925768344455326}. Best is trial 17 with value: 0.9012206542961001.\n",
      "[I 2025-03-10 16:39:29,466] Trial 19 finished with value: 0.9012219770684189 and parameters: {'weight_logistic': 0.5371968986533222, 'weight_xgb': 0.18287398800282545, 'weight_catboost': 0.7435995933334711}. Best is trial 19 with value: 0.9012219770684189.\n",
      "[I 2025-03-10 16:39:29,475] Trial 20 finished with value: 0.9010883770641861 and parameters: {'weight_logistic': 0.557678454193016, 'weight_xgb': 0.35245989986474796, 'weight_catboost': 0.7343295438928731}. Best is trial 19 with value: 0.9012219770684189.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating meta-learner: catboost...\n",
      "catboost - Accuracy: 0.8056, AUC: 0.9014\n",
      "\n",
      "Optimizing ensemble weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:39:29,482] Trial 21 finished with value: 0.9012166859791424 and parameters: {'weight_logistic': 0.46595297551731674, 'weight_xgb': 0.18840077850109244, 'weight_catboost': 0.6934210527855422}. Best is trial 19 with value: 0.9012219770684189.\n",
      "[I 2025-03-10 16:39:29,489] Trial 22 finished with value: 0.9012166859791426 and parameters: {'weight_logistic': 0.4640067257867834, 'weight_xgb': 0.19607216597585475, 'weight_catboost': 0.7140192571371862}. Best is trial 19 with value: 0.9012219770684189.\n",
      "[I 2025-03-10 16:39:29,498] Trial 23 finished with value: 0.9012272681576956 and parameters: {'weight_logistic': 0.5811604260557818, 'weight_xgb': 0.21425862547067503, 'weight_catboost': 0.8080558153589704}. Best is trial 23 with value: 0.9012272681576956.\n",
      "[I 2025-03-10 16:39:29,535] Trial 24 finished with value: 0.9009984285464847 and parameters: {'weight_logistic': 0.6338599818141065, 'weight_xgb': 0.4907446492620352, 'weight_catboost': 0.8543887859821477}. Best is trial 23 with value: 0.9012272681576956.\n",
      "[I 2025-03-10 16:39:29,549] Trial 25 finished with value: 0.9011373196399941 and parameters: {'weight_logistic': 0.5686957657925823, 'weight_xgb': 0.33848406358942895, 'weight_catboost': 0.841164881172683}. Best is trial 23 with value: 0.9012272681576956.\n",
      "[I 2025-03-10 16:39:29,565] Trial 26 finished with value: 0.9012815018227802 and parameters: {'weight_logistic': 0.6810434285757664, 'weight_xgb': 0.2006136845251347, 'weight_catboost': 0.995374329454469}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,573] Trial 27 finished with value: 0.9012484325148018 and parameters: {'weight_logistic': 0.6988481938007264, 'weight_xgb': 0.2099400104360761, 'weight_catboost': 0.8339028323623728}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,584] Trial 28 finished with value: 0.9011333513230367 and parameters: {'weight_logistic': 0.7274284824058671, 'weight_xgb': 0.3669180501089261, 'weight_catboost': 0.9926544330971434}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,591] Trial 29 finished with value: 0.9009732958724213 and parameters: {'weight_logistic': 0.913705360684469, 'weight_xgb': 0.503449589345556, 'weight_catboost': 0.8194870831342792}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,597] Trial 30 finished with value: 0.9012457869701634 and parameters: {'weight_logistic': 0.6751264334973369, 'weight_xgb': 0.23755938914666808, 'weight_catboost': 0.9959294185421481}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,604] Trial 31 finished with value: 0.9012590146933549 and parameters: {'weight_logistic': 0.6814617639757083, 'weight_xgb': 0.2190249580771816, 'weight_catboost': 0.9982743208735391}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,611] Trial 32 finished with value: 0.9011994899389938 and parameters: {'weight_logistic': 0.6984286788896246, 'weight_xgb': 0.29474706412969154, 'weight_catboost': 0.9909535111879815}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,617] Trial 33 finished with value: 0.9012563691487167 and parameters: {'weight_logistic': 0.8678032929256634, 'weight_xgb': 0.16344213412318429, 'weight_catboost': 0.9268214548339611}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,624] Trial 34 finished with value: 0.9012471097424827 and parameters: {'weight_logistic': 0.8628525622110883, 'weight_xgb': 0.16455997825060154, 'weight_catboost': 0.9248974047983566}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,630] Trial 35 finished with value: 0.9010923453811437 and parameters: {'weight_logistic': 0.7709994206387552, 'weight_xgb': 0.39098524745346525, 'weight_catboost': 0.892805252861996}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,637] Trial 36 finished with value: 0.9009243532966131 and parameters: {'weight_logistic': 0.8558890795494245, 'weight_xgb': 0.7288676357125903, 'weight_catboost': 0.9502973490747296}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,643] Trial 37 finished with value: 0.901211394889866 and parameters: {'weight_logistic': 0.6375651101963825, 'weight_xgb': 0.15863433505800534, 'weight_catboost': 0.6631723345848227}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,650] Trial 38 finished with value: 0.9011770028095684 and parameters: {'weight_logistic': 0.7617728060759922, 'weight_xgb': 0.29059977484057364, 'weight_catboost': 0.8790258050629096}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,657] Trial 39 finished with value: 0.9010936681534627 and parameters: {'weight_logistic': 0.9794294665850025, 'weight_xgb': 0.4275463990484327, 'weight_catboost': 0.9435961579342824}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,663] Trial 40 finished with value: 0.9006505394265518 and parameters: {'weight_logistic': 0.8287442829129984, 'weight_xgb': 0.9810818599488497, 'weight_catboost': 0.7894340730543634}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,670] Trial 41 finished with value: 0.9012524008317593 and parameters: {'weight_logistic': 0.8872104817448563, 'weight_xgb': 0.16760778071246105, 'weight_catboost': 0.9227785931843964}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,676] Trial 42 finished with value: 0.9012669513272697 and parameters: {'weight_logistic': 0.9161930516316831, 'weight_xgb': 0.14954458720401698, 'weight_catboost': 0.9567550407342749}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,682] Trial 43 finished with value: 0.9012682740995888 and parameters: {'weight_logistic': 0.9173649296312594, 'weight_xgb': 0.1415766440573062, 'weight_catboost': 0.9594662923509386}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,689] Trial 44 finished with value: 0.9012246226130574 and parameters: {'weight_logistic': 0.9243089692459463, 'weight_xgb': 0.12965973501582606, 'weight_catboost': 0.8757287575124779}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,698] Trial 45 finished with value: 0.9010592760731653 and parameters: {'weight_logistic': 0.9267137024394493, 'weight_xgb': 0.2267399042715671, 'weight_catboost': 0.6403531473038949}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,716] Trial 46 finished with value: 0.9012338820192912 and parameters: {'weight_logistic': 0.9983723655010281, 'weight_xgb': 0.14954659657091718, 'weight_catboost': 0.9587529685956253}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,723] Trial 47 finished with value: 0.9011479018185475 and parameters: {'weight_logistic': 0.8039996217488423, 'weight_xgb': 0.32217825220133545, 'weight_catboost': 0.902813745144235}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,731] Trial 48 finished with value: 0.9011518701355048 and parameters: {'weight_logistic': 0.760550883083857, 'weight_xgb': 0.26343011415110673, 'weight_catboost': 0.7783312429512558}. Best is trial 26 with value: 0.9012815018227802.\n",
      "[I 2025-03-10 16:39:29,738] Trial 49 finished with value: 0.9012881156843758 and parameters: {'weight_logistic': 0.8312090584316808, 'weight_xgb': 0.14596170758219681, 'weight_catboost': 0.9608962908013288}. Best is trial 49 with value: 0.9012881156843758.\n",
      "[I 2025-03-10 16:39:29,744] Trial 50 finished with value: 0.9012947295459717 and parameters: {'weight_logistic': 0.832159855237603, 'weight_xgb': 0.13358247899159464, 'weight_catboost': 0.9692603309289987}. Best is trial 50 with value: 0.9012947295459717.\n",
      "[I 2025-03-10 16:39:29,751] Trial 51 finished with value: 0.9013436721217797 and parameters: {'weight_logistic': 0.8281164674424425, 'weight_xgb': 0.10013865102848889, 'weight_catboost': 0.9660314947231512}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,758] Trial 52 finished with value: 0.9012841473674185 and parameters: {'weight_logistic': 0.8306885377486778, 'weight_xgb': 0.13514559672247264, 'weight_catboost': 0.9550411152566554}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,765] Trial 53 finished with value: 0.9013344127155458 and parameters: {'weight_logistic': 0.8295846008576306, 'weight_xgb': 0.1082083022510381, 'weight_catboost': 0.9614580222544891}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,771] Trial 54 finished with value: 0.9013158939030779 and parameters: {'weight_logistic': 0.8238017923287732, 'weight_xgb': 0.10029186650399602, 'weight_catboost': 0.8838629260749366}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,778] Trial 55 finished with value: 0.9012920840013333 and parameters: {'weight_logistic': 0.8158457275091167, 'weight_xgb': 0.11319888615893665, 'weight_catboost': 0.8775293192619805}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,785] Trial 56 finished with value: 0.9013000206352482 and parameters: {'weight_logistic': 0.7810171124527152, 'weight_xgb': 0.11719273662273125, 'weight_catboost': 0.8761050396201425}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,791] Trial 57 finished with value: 0.9010923453811437 and parameters: {'weight_logistic': 0.782160602745456, 'weight_xgb': 0.11284333969042523, 'weight_catboost': 0.5352706670171218}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,799] Trial 58 finished with value: 0.9013026661798865 and parameters: {'weight_logistic': 0.732390331801069, 'weight_xgb': 0.10655629919070875, 'weight_catboost': 0.8653566211504454}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,809] Trial 59 finished with value: 0.901289438456695 and parameters: {'weight_logistic': 0.7348719325247106, 'weight_xgb': 0.10470149906039175, 'weight_catboost': 0.774402252064053}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,856] Trial 60 finished with value: 0.9010222384482293 and parameters: {'weight_logistic': 0.7446235326022084, 'weight_xgb': 0.18147584362362823, 'weight_catboost': 0.4868615583260156}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,879] Trial 61 finished with value: 0.9013066344968438 and parameters: {'weight_logistic': 0.813576363695742, 'weight_xgb': 0.10946748925137192, 'weight_catboost': 0.8587642958171674}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,890] Trial 62 finished with value: 0.9013026661798865 and parameters: {'weight_logistic': 0.7984149440690158, 'weight_xgb': 0.10205940237971142, 'weight_catboost': 0.8701298415370299}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,897] Trial 63 finished with value: 0.9009203849796557 and parameters: {'weight_logistic': 0.7972248976685314, 'weight_xgb': 0.6370409562079635, 'weight_catboost': 0.8594778066754873}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,905] Trial 64 finished with value: 0.9011703889479727 and parameters: {'weight_logistic': 0.8770815397327272, 'weight_xgb': 0.19799827804831266, 'weight_catboost': 0.8093279780588927}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,912] Trial 65 finished with value: 0.9012365275639295 and parameters: {'weight_logistic': 0.9544144927083882, 'weight_xgb': 0.10621504070442983, 'weight_catboost': 0.9010127385613933}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,920] Trial 66 finished with value: 0.9012391731085678 and parameters: {'weight_logistic': 0.7853479303640093, 'weight_xgb': 0.18402273380167983, 'weight_catboost': 0.8511711070953523}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,927] Trial 67 finished with value: 0.9012259453853766 and parameters: {'weight_logistic': 0.6024726979808315, 'weight_xgb': 0.10087836494802087, 'weight_catboost': 0.5832360356683488}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,933] Trial 68 finished with value: 0.901183616671164 and parameters: {'weight_logistic': 0.6530579473387648, 'weight_xgb': 0.2467831243086081, 'weight_catboost': 0.7450014942303713}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,940] Trial 69 finished with value: 0.9012934067736526 and parameters: {'weight_logistic': 0.7117932233367223, 'weight_xgb': 0.1308994902189903, 'weight_catboost': 0.9084038541785465}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,947] Trial 70 finished with value: 0.9012285909300148 and parameters: {'weight_logistic': 0.5142075497036546, 'weight_xgb': 0.21651740238256023, 'weight_catboost': 0.8316966182594899}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,954] Trial 71 finished with value: 0.901260337465674 and parameters: {'weight_logistic': 0.8453668170844246, 'weight_xgb': 0.1312548888533974, 'weight_catboost': 0.8641367250978329}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,960] Trial 72 finished with value: 0.9012576919210358 and parameters: {'weight_logistic': 0.8854316280349642, 'weight_xgb': 0.18204878522554757, 'weight_catboost': 0.9230219487856252}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,967] Trial 73 finished with value: 0.9012815018227802 and parameters: {'weight_logistic': 0.8025470043314141, 'weight_xgb': 0.1622023406724073, 'weight_catboost': 0.9722250228191232}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,974] Trial 74 finished with value: 0.9012960523182907 and parameters: {'weight_logistic': 0.8501147979836633, 'weight_xgb': 0.10032595030503726, 'weight_catboost': 0.8911615546869035}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,983] Trial 75 finished with value: 0.9012907612290143 and parameters: {'weight_logistic': 0.7487524286625591, 'weight_xgb': 0.10242685243926433, 'weight_catboost': 0.801844191098277}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,990] Trial 76 finished with value: 0.9012299137023339 and parameters: {'weight_logistic': 0.8913649183078824, 'weight_xgb': 0.16968734310182487, 'weight_catboost': 0.8916046108376128}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:29,996] Trial 77 finished with value: 0.9008026582432525 and parameters: {'weight_logistic': 0.9494833609694657, 'weight_xgb': 0.8267173396806964, 'weight_catboost': 0.8288114527475665}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,003] Trial 78 finished with value: 0.9010989592427392 and parameters: {'weight_logistic': 0.4106907465424775, 'weight_xgb': 0.277213699131727, 'weight_catboost': 0.7597347311803493}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,010] Trial 79 finished with value: 0.9012960523182908 and parameters: {'weight_logistic': 0.8480602774335769, 'weight_xgb': 0.13084576132957523, 'weight_catboost': 0.9278076011438223}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,016] Trial 80 finished with value: 0.901268274099589 and parameters: {'weight_logistic': 0.7758700924291633, 'weight_xgb': 0.2020660737839951, 'weight_catboost': 0.9329006991944245}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,023] Trial 81 finished with value: 0.901277533505823 and parameters: {'weight_logistic': 0.8510720280490244, 'weight_xgb': 0.12761793884851277, 'weight_catboost': 0.8869724733097866}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,030] Trial 82 finished with value: 0.9012563691487165 and parameters: {'weight_logistic': 0.8113567887395132, 'weight_xgb': 0.1553871816225697, 'weight_catboost': 0.850718668118607}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,037] Trial 83 finished with value: 0.901298697862929 and parameters: {'weight_logistic': 0.8592189012571143, 'weight_xgb': 0.12171738703390485, 'weight_catboost': 0.9232052768163721}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,043] Trial 84 finished with value: 0.9007986899262952 and parameters: {'weight_logistic': 0.8933791994679283, 'weight_xgb': 0.23010041262732783, 'weight_catboost': 0.3593053456995412}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,050] Trial 85 finished with value: 0.9013026661798864 and parameters: {'weight_logistic': 0.73034883056615, 'weight_xgb': 0.13012410401609983, 'weight_catboost': 0.9801977363022327}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,057] Trial 86 finished with value: 0.90128150182278 and parameters: {'weight_logistic': 0.7201618725886124, 'weight_xgb': 0.15435591742363822, 'weight_catboost': 0.9787611763865988}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,063] Trial 87 finished with value: 0.9013079572691629 and parameters: {'weight_logistic': 0.7742377452163468, 'weight_xgb': 0.12193417292534148, 'weight_catboost': 0.9421511462985603}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,069] Trial 88 finished with value: 0.9013092800414821 and parameters: {'weight_logistic': 0.6604027440208651, 'weight_xgb': 0.17632061940317958, 'weight_catboost': 0.9819287184767106}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,076] Trial 89 finished with value: 0.9013158939030779 and parameters: {'weight_logistic': 0.6618936781650857, 'weight_xgb': 0.17393343315612422, 'weight_catboost': 0.9976295527920345}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,083] Trial 90 finished with value: 0.9012881156843758 and parameters: {'weight_logistic': 0.663071790068331, 'weight_xgb': 0.17373924112674252, 'weight_catboost': 0.9394581891451855}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,090] Trial 91 finished with value: 0.9012841473674185 and parameters: {'weight_logistic': 0.6141082658178659, 'weight_xgb': 0.20456595870379707, 'weight_catboost': 0.9823072361449783}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,096] Trial 92 finished with value: 0.9012920840013334 and parameters: {'weight_logistic': 0.7045440096056306, 'weight_xgb': 0.18684798103339004, 'weight_catboost': 0.9970591572536236}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,103] Trial 93 finished with value: 0.9012867929120567 and parameters: {'weight_logistic': 0.7527975900119684, 'weight_xgb': 0.15726044300667072, 'weight_catboost': 0.978685187507502}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,110] Trial 94 finished with value: 0.9012180087514616 and parameters: {'weight_logistic': 0.7324683300340389, 'weight_xgb': 0.24452434282972924, 'weight_catboost': 0.9480812869061483}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,117] Trial 95 finished with value: 0.9012907612290142 and parameters: {'weight_logistic': 0.6900197732001264, 'weight_xgb': 0.14523616028438618, 'weight_catboost': 0.9073354270352861}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,154] Trial 96 finished with value: 0.9010354661714208 and parameters: {'weight_logistic': 0.6541254441014667, 'weight_xgb': 0.5001554128597364, 'weight_catboost': 0.9475791618534386}. Best is trial 51 with value: 0.9013436721217797.\n",
      "[I 2025-03-10 16:39:30,166] Trial 97 finished with value: 0.90137409588512 and parameters: {'weight_logistic': 0.5984154466126697, 'weight_xgb': 0.12161419516437502, 'weight_catboost': 0.9719793804989076}. Best is trial 97 with value: 0.90137409588512.\n",
      "[I 2025-03-10 16:39:30,176] Trial 98 finished with value: 0.9013595453896094 and parameters: {'weight_logistic': 0.5716857033208952, 'weight_xgb': 0.11842821001753771, 'weight_catboost': 0.9170205535586826}. Best is trial 97 with value: 0.90137409588512.\n",
      "[I 2025-03-10 16:39:30,183] Trial 99 finished with value: 0.9013344127155457 and parameters: {'weight_logistic': 0.5894001522930845, 'weight_xgb': 0.14429596192674063, 'weight_catboost': 0.9986311661346948}. Best is trial 97 with value: 0.90137409588512.\n",
      "[I 2025-03-10 16:39:30,190] A new study created in memory with name: no-name-9ad233fc-2d66-41c9-8959-ca8ec378d860\n",
      "[I 2025-03-10 16:39:30,192] Trial 0 finished with value: 0.8062104657849338 and parameters: {'threshold': 0.6497648996086798}. Best is trial 0 with value: 0.8062104657849338.\n",
      "[I 2025-03-10 16:39:30,194] Trial 1 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.6371982559505697}. Best is trial 1 with value: 0.8102357676825762.\n",
      "[I 2025-03-10 16:39:30,202] Trial 2 finished with value: 0.8085106382978723 and parameters: {'threshold': 0.4455829499618429}. Best is trial 1 with value: 0.8102357676825762.\n",
      "[I 2025-03-10 16:39:30,205] Trial 3 finished with value: 0.7952846463484762 and parameters: {'threshold': 0.31673630098280287}. Best is trial 1 with value: 0.8102357676825762.\n",
      "[I 2025-03-10 16:39:30,207] Trial 4 finished with value: 0.80448533640023 and parameters: {'threshold': 0.4880828110479669}. Best is trial 1 with value: 0.8102357676825762.\n",
      "[I 2025-03-10 16:39:30,208] Trial 5 finished with value: 0.7947096032202415 and parameters: {'threshold': 0.311083671850846}. Best is trial 1 with value: 0.8102357676825762.\n",
      "[I 2025-03-10 16:39:30,208] Trial 6 finished with value: 0.8056354226566993 and parameters: {'threshold': 0.46480760794761267}. Best is trial 1 with value: 0.8102357676825762.\n",
      "[I 2025-03-10 16:39:30,209] Trial 7 finished with value: 0.8073605520414031 and parameters: {'threshold': 0.6868496557142432}. Best is trial 1 with value: 0.8102357676825762.\n",
      "[I 2025-03-10 16:39:30,210] Trial 8 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5823690867557088}. Best is trial 8 with value: 0.8108108108108109.\n",
      "[I 2025-03-10 16:39:30,211] Trial 9 finished with value: 0.8021851638872916 and parameters: {'threshold': 0.4806304932793112}. Best is trial 8 with value: 0.8108108108108109.\n",
      "[I 2025-03-10 16:39:30,214] Trial 10 finished with value: 0.8096607245543416 and parameters: {'threshold': 0.5745748538290861}. Best is trial 8 with value: 0.8108108108108109.\n",
      "[I 2025-03-10 16:39:30,216] Trial 11 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.5950222501548064}. Best is trial 8 with value: 0.8108108108108109.\n",
      "[I 2025-03-10 16:39:30,219] Trial 12 finished with value: 0.8096607245543416 and parameters: {'threshold': 0.5747716127355947}. Best is trial 8 with value: 0.8108108108108109.\n",
      "[I 2025-03-10 16:39:30,221] Trial 13 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.6259301295291252}. Best is trial 8 with value: 0.8108108108108109.\n",
      "[I 2025-03-10 16:39:30,224] Trial 14 finished with value: 0.8125359401955147 and parameters: {'threshold': 0.5447347139101993}. Best is trial 14 with value: 0.8125359401955147.\n",
      "[I 2025-03-10 16:39:30,226] Trial 15 finished with value: 0.8096607245543416 and parameters: {'threshold': 0.5271384066071904}. Best is trial 14 with value: 0.8125359401955147.\n",
      "[I 2025-03-10 16:39:30,229] Trial 16 finished with value: 0.8039102932719954 and parameters: {'threshold': 0.3937436778084369}. Best is trial 14 with value: 0.8125359401955147.\n",
      "[I 2025-03-10 16:39:30,231] Trial 17 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5287810420806677}. Best is trial 14 with value: 0.8125359401955147.\n",
      "[I 2025-03-10 16:39:30,234] Trial 18 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.542185184345236}. Best is trial 18 with value: 0.8131109833237493.\n",
      "[I 2025-03-10 16:39:30,236] Trial 19 finished with value: 0.80448533640023 and parameters: {'threshold': 0.40952720567193635}. Best is trial 18 with value: 0.8131109833237493.\n",
      "[I 2025-03-10 16:39:30,239] Trial 20 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5306254803746289}. Best is trial 18 with value: 0.8131109833237493.\n",
      "[I 2025-03-10 16:39:30,241] Trial 21 finished with value: 0.8090856814261069 and parameters: {'threshold': 0.5701098466274499}. Best is trial 18 with value: 0.8131109833237493.\n",
      "[I 2025-03-10 16:39:30,244] Trial 22 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.5385514711980094}. Best is trial 18 with value: 0.8131109833237493.\n",
      "[I 2025-03-10 16:39:30,247] Trial 23 finished with value: 0.8136860264519838 and parameters: {'threshold': 0.5357267725938728}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,249] Trial 24 finished with value: 0.8079355951696378 and parameters: {'threshold': 0.5078929604015391}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,251] Trial 25 finished with value: 0.8079355951696378 and parameters: {'threshold': 0.43494681597589246}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,255] Trial 26 finished with value: 0.8067855089131685 and parameters: {'threshold': 0.5047484286327547}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,257] Trial 27 finished with value: 0.8090856814261069 and parameters: {'threshold': 0.6098503026037528}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,260] Trial 28 finished with value: 0.8125359401955147 and parameters: {'threshold': 0.547067676708877}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,262] Trial 29 finished with value: 0.8056354226566993 and parameters: {'threshold': 0.6537970643221482}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,265] Trial 30 finished with value: 0.8085106382978723 and parameters: {'threshold': 0.6658169705619097}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,268] Trial 31 finished with value: 0.8125359401955147 and parameters: {'threshold': 0.5486061448739931}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,270] Trial 32 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5596202514221097}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,274] Trial 33 finished with value: 0.8090856814261069 and parameters: {'threshold': 0.5121605199082894}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,278] Trial 34 finished with value: 0.8090856814261069 and parameters: {'threshold': 0.6103915787283305}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,281] Trial 35 finished with value: 0.8050603795284647 and parameters: {'threshold': 0.45765978519613215}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,284] Trial 36 finished with value: 0.8027602070155262 and parameters: {'threshold': 0.4837719166615939}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,287] Trial 37 finished with value: 0.7987349051178838 and parameters: {'threshold': 0.3553997407036431}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,290] Trial 38 finished with value: 0.8136860264519838 and parameters: {'threshold': 0.5423094227677668}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,293] Trial 39 finished with value: 0.8113858539390454 and parameters: {'threshold': 0.5982771475994425}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,295] Trial 40 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.6287238174627406}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,298] Trial 41 finished with value: 0.8125359401955147 and parameters: {'threshold': 0.5469759928835852}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,300] Trial 42 finished with value: 0.8079355951696378 and parameters: {'threshold': 0.5220021590334275}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,303] Trial 43 finished with value: 0.8033352501437608 and parameters: {'threshold': 0.49584639412901715}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,305] Trial 44 finished with value: 0.8039102932719954 and parameters: {'threshold': 0.46757327111587543}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,308] Trial 45 finished with value: 0.8113858539390454 and parameters: {'threshold': 0.5564414223697912}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,311] Trial 46 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.5375062653832708}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,327] Trial 47 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.5883210373755989}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,330] Trial 48 finished with value: 0.8090856814261069 and parameters: {'threshold': 0.5704912304415982}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,333] Trial 49 finished with value: 0.8021851638872916 and parameters: {'threshold': 0.4762722875576909}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,335] Trial 50 finished with value: 0.8085106382978723 and parameters: {'threshold': 0.44588343107105977}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,338] Trial 51 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.53004354833069}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,341] Trial 52 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.5390901012292528}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,344] Trial 53 finished with value: 0.8033352501437608 and parameters: {'threshold': 0.495183939048489}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,347] Trial 54 finished with value: 0.8085106382978723 and parameters: {'threshold': 0.5160125593672598}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,350] Trial 55 finished with value: 0.8136860264519838 and parameters: {'threshold': 0.5402540971009933}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,353] Trial 56 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5613576128621897}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,355] Trial 57 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5789881183826147}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,358] Trial 58 finished with value: 0.8096607245543416 and parameters: {'threshold': 0.6043974997871958}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,362] Trial 59 finished with value: 0.8079355951696378 and parameters: {'threshold': 0.5052513709896799}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,365] Trial 60 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.5366370444693327}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,368] Trial 61 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.541894660434645}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,370] Trial 62 finished with value: 0.8085106382978723 and parameters: {'threshold': 0.518878475911664}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,374] Trial 63 finished with value: 0.8113858539390454 and parameters: {'threshold': 0.5577199357478405}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,379] Trial 64 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5316998708806193}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,382] Trial 65 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.584631606234817}. Best is trial 23 with value: 0.8136860264519838.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized meta-learner weights:\n",
      "logistic: 0.3537\n",
      "xgb: 0.0719\n",
      "catboost: 0.5745\n",
      "\n",
      "===== DEFAULT THRESHOLD EVALUATION =====\n",
      "Default Threshold: 0.5\n",
      "Validation Accuracy: 0.8045\n",
      "Validation AUC: 0.9014\n",
      "\n",
      "Default Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80       863\n",
      "           1       0.81      0.80      0.81       876\n",
      "\n",
      "    accuracy                           0.80      1739\n",
      "   macro avg       0.80      0.80      0.80      1739\n",
      "weighted avg       0.80      0.80      0.80      1739\n",
      "\n",
      "\n",
      "===== THRESHOLD OPTIMIZATION WITH OPTUNA =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 16:39:30,386] Trial 66 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.5650679315749679}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,390] Trial 67 finished with value: 0.8136860264519838 and parameters: {'threshold': 0.540150882766013}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,395] Trial 68 finished with value: 0.8085106382978723 and parameters: {'threshold': 0.5191711901279389}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,399] Trial 69 finished with value: 0.80448533640023 and parameters: {'threshold': 0.5018992585050437}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,403] Trial 70 finished with value: 0.8050603795284647 and parameters: {'threshold': 0.4891800413988359}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,408] Trial 71 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.5388645049332556}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,412] Trial 72 finished with value: 0.8125359401955147 and parameters: {'threshold': 0.547719804105312}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,419] Trial 73 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.5760853722616796}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,423] Trial 74 finished with value: 0.8113858539390454 and parameters: {'threshold': 0.5552158809351174}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,426] Trial 75 finished with value: 0.8090856814261069 and parameters: {'threshold': 0.5252199479910541}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,429] Trial 76 finished with value: 0.8090856814261069 and parameters: {'threshold': 0.5115221374155882}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,432] Trial 77 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.5902155633991328}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,435] Trial 78 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.5407057932520354}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,438] Trial 79 finished with value: 0.8113858539390454 and parameters: {'threshold': 0.5527232487884102}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,440] Trial 80 finished with value: 0.8096607245543416 and parameters: {'threshold': 0.5724704540566246}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,443] Trial 81 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.5372418868859021}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,446] Trial 82 finished with value: 0.8131109833237493 and parameters: {'threshold': 0.537172672259105}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,448] Trial 83 finished with value: 0.8096607245543416 and parameters: {'threshold': 0.5276328684239688}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,451] Trial 84 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5613576494413187}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,454] Trial 85 finished with value: 0.8085106382978723 and parameters: {'threshold': 0.513209790822171}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,470] Trial 86 finished with value: 0.8125359401955147 and parameters: {'threshold': 0.5481912347241434}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,481] Trial 87 finished with value: 0.8033352501437608 and parameters: {'threshold': 0.49491762715532045}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,497] Trial 88 finished with value: 0.8039102932719954 and parameters: {'threshold': 0.47155376219970924}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,500] Trial 89 finished with value: 0.8113858539390454 and parameters: {'threshold': 0.5322551022226463}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,507] Trial 90 finished with value: 0.8039102932719954 and parameters: {'threshold': 0.48621663946400406}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,511] Trial 91 finished with value: 0.8102357676825762 and parameters: {'threshold': 0.565670529832856}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,514] Trial 92 finished with value: 0.8085106382978723 and parameters: {'threshold': 0.5208607139730018}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,517] Trial 93 finished with value: 0.81196089706728 and parameters: {'threshold': 0.5499111401155058}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,521] Trial 94 finished with value: 0.8136860264519838 and parameters: {'threshold': 0.540351001309438}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,523] Trial 95 finished with value: 0.8079355951696378 and parameters: {'threshold': 0.5076275374521114}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,526] Trial 96 finished with value: 0.8079355951696378 and parameters: {'threshold': 0.6960753707888032}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,529] Trial 97 finished with value: 0.8136860264519838 and parameters: {'threshold': 0.5351869312360893}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,532] Trial 98 finished with value: 0.7981598619896493 and parameters: {'threshold': 0.3395568759468618}. Best is trial 23 with value: 0.8136860264519838.\n",
      "[I 2025-03-10 16:39:30,534] Trial 99 finished with value: 0.8108108108108109 and parameters: {'threshold': 0.5803401688162113}. Best is trial 23 with value: 0.8136860264519838.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.5357\n",
      "Optimal Validation Accuracy: 0.8137\n",
      "Improvement: 0.9201%\n",
      "\n",
      "Optimized Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       863\n",
      "           1       0.83      0.79      0.81       876\n",
      "\n",
      "    accuracy                           0.81      1739\n",
      "   macro avg       0.81      0.81      0.81      1739\n",
      "weighted avg       0.81      0.81      0.81      1739\n",
      "\n",
      "\n",
      "Total ensemble building time: 1217.56 seconds (20.29 minutes)\n",
      "\n",
      "===== FINAL EVALUATION =====\n",
      "Default Threshold (0.5) Accuracy: 0.8045\n",
      "Optimal Threshold (0.5357) Accuracy: 0.8137\n",
      "Improvement: 0.9201%\n",
      "\n",
      "Final Classification Report (Optimized):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       863\n",
      "           1       0.83      0.79      0.81       876\n",
      "\n",
      "    accuracy                           0.81      1739\n",
      "   macro avg       0.81      0.81      0.81      1739\n",
      "weighted avg       0.81      0.81      0.81      1739\n",
      "\n",
      "\n",
      "===== GENERATING SUBMISSION =====\n",
      "Submission file created: submission.csv\n",
      "Using optimal threshold: 0.5357\n"
     ]
    }
   ],
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
